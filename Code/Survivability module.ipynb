{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_by_index(X,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    return(X)\n",
    "\n",
    "def dataSetting(dropCol,FILE_FOLDER = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    function to read the pkl from from datasource\n",
    "        1. Remove dx_date that is NULL.\n",
    "        2. Drop all rows where crucial fields for X_features are NULL.\n",
    "        3. Convert Date columns into datetime format\n",
    "        4. Derive OS, CSS, DFS days based on dx_date\n",
    "        5. Create status column to indicate if the patient is dead or alive base on if death_age exists\n",
    "    '''\n",
    "    df = pd.read_pickle(FILE_FOLDER + \"clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "    to_drop = df[df['dx_date']==\"NA\"].index\n",
    "    df = drop_by_index(df,to_drop)\n",
    "\n",
    "    df.drop(columns=dropCol,inplace = True)\n",
    "\n",
    "    # drop all rows where dates are null\n",
    "    df.dropna(axis=0,\\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date','size_precise', 'nodespos'],\\\n",
    "                    inplace=True)\n",
    "\n",
    "    # convert all datetime in dataframe into dateime format for processing\n",
    "    df[\"Date_for_DFS\"] = pd.to_datetime(df[\"Date_for_DFS\"])\n",
    "    df[\"Date_for_OS\"] = pd.to_datetime(df[\"Date_for_OS\"])\n",
    "    df[\"Date_for_CSS\"] = pd.to_datetime(df[\"Date_for_CSS\"])\n",
    "    df[\"dx_date\"] = pd.to_datetime(df[\"dx_date\"])\n",
    "    df['last_seen']= pd.to_datetime(df[\"dx_date\"])\n",
    "\n",
    "    # calculate in days\n",
    "    df[\"DFS_days\"] = (df[\"Date_for_DFS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"OS_days\"] = (df[\"Date_for_OS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"CSS_days\"] = (df[\"Date_for_CSS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "\n",
    "    # alive or dead\n",
    "    df['status'] = np.where(df['death_age'].isnull(), False, True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ComputeYears(df, Year_list):\n",
    "    '''\n",
    "    Create a list to contain df for different years of survival\n",
    "    The df will filter those patient that has deceased or days of survival longer than the defined years.\n",
    "    '''\n",
    "\n",
    "    df_dict = {}\n",
    "\n",
    "    for i in Year_list:\n",
    "        tmp = {}\n",
    "        for x in list([\"DFS\", \"CSS\", \"OS\"]):\n",
    "            df['{}_{}_years'.format(x, i)] = np.where(\n",
    "                                                      np.logical_or(df['death_age'] > 0,\\\n",
    "                                                      df['{}_days'.format(x)]/(365.25*i) >= i),\\\n",
    "                                                      True,False)\n",
    "            tmp[x] = df[df['{}_{}_years'.format(x, i)] == True]\n",
    "        df_dict['{}_years'.format(i)] = tmp\n",
    "    return df_dict\n",
    "\n",
    "def settingXY(df_dict, X_features, Y_features, YEAR, STYPE, OHE_LOCATION = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    This function returns the X and Y features need for model training\n",
    "        - The function also generates one pkl that contains the One Hot Encoder for new raw data \n",
    "    \n",
    "    X_features = features to use for X\n",
    "    Y_features = features to use for Y \n",
    "    YEAR = years of patient record interested\n",
    "    SYTPE = survival type (OS, DFS, CSS)\n",
    "    OHE_LOCATION = location to store the pkl file\n",
    "    '''\n",
    "    import pickle\n",
    "    \n",
    "    DF_TO_USE = df_dict['{}_years'.format(YEAR)][STYPE]\n",
    "\n",
    "    X = DF_TO_USE[X_features]\n",
    "    Y = DF_TO_USE[Y_features]\n",
    "\n",
    "    # convert to int since some fields fro age_@_dx is null\n",
    "    X.loc[:,\"Age_@_Dx\"] = X[\"Age_@_Dx\"].astype(\"int16\")\n",
    "\n",
    "    # Save enconder so that we can OHE new data\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    \n",
    "    # OHE for probability\n",
    "    X = enc.transform(X)\n",
    "    with open(OHE_LOCATION + 'encoder.pickle', 'wb') as f:\n",
    "        pickle.dump(enc, f) \n",
    "                  \n",
    "    # convert Y to structured array\n",
    "    s = Y.dtypes\n",
    "    Y = np.array([tuple(x) for x in Y.values], dtype=list(zip(s.index, s)))\n",
    "   \n",
    "    return X, Y\n",
    "\n",
    "def train_test(X, Y, test_size = 0.33, random_state = 42,):\n",
    "    '''\n",
    "    Splitting the dataset into the Training set and Test set\n",
    "    '''\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,  test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def Cox(X_train,Y_train,alpha = 1e-4, verbose = 0):\n",
    "        \n",
    "    # since features are highly corelated, reducing alpha values to smaller values allows the learning\n",
    "    model = CoxPHSurvivalAnalysis(alpha = alpha, verbose = verbose)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    '''\n",
    "    In survival analysis, the hazard ratio (HR) is the ratio of the hazard rates corresponding\n",
    "    to the conditions described by two levels of an explanatory variable. \n",
    "        For example, in a drug study, the treated population may die at twice the rate per unit time\n",
    "        as the control population. The hazard ratio would be 2, indicating higher hazard of death from the treatment. \n",
    "        Or in another study, men receiving the same treatment may suffer a certain complication ten times more\n",
    "        frequently per unit time than women, giving a hazard ratio of 10. - wiki\n",
    "    '''\n",
    "    tmp = pd.Series(model.coef_, index=X_train.columns)\n",
    "    tmp = tmp.to_frame(\"Log Hazarad Ratio\")\n",
    "    tmp = tmp.sort_values(by=['Log Hazarad Ratio'])\n",
    "    \n",
    "    return model, tmp\n",
    "\n",
    "def fit_and_score_features(X, y):\n",
    "    '''\n",
    "    Based on the Cox model, rank the scores of each feature to understand which X features plays the key role in\n",
    "    modelling\n",
    "    '''\n",
    "    n_features = X.shape[1]\n",
    "    scores = np.empty(n_features)\n",
    "    m = CoxPHSurvivalAnalysis(alpha = 1e-4)\n",
    "    for j in range(n_features):\n",
    "        Xj = X[:, j:j+1]\n",
    "        m.fit(Xj, y)\n",
    "        scores[j] = m.score(Xj, y)\n",
    "    return scores\n",
    "\n",
    "def plotGraph(df, YEAR, STYPE, UNITS = 0, ):\n",
    "    '''\n",
    "    function to plot the graph\n",
    "    UNITS: {0: days, 1: years}\n",
    "    '''\n",
    "    if UNITS == 1:\n",
    "        unit = \"Years\"\n",
    "    else:\n",
    "        unit = \"Days\"\n",
    "        \n",
    "    time, survival_prob = kaplan_meier_estimator(df['{}_years'.format(YEAR)][STYPE]['status'], \n",
    "                                                 df['{}_years'.format(YEAR)][STYPE]['{}_days'.format(STYPE)])\n",
    "    \n",
    "    if UNITS == 1:\n",
    "        time = time/365.25\n",
    "    plt.step(time, survival_prob, where=\"post\")\n",
    "    \n",
    "    plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "    plt.xlabel(\"time $t$ ({})\".format(unit))\n",
    "    plt.title(\"{} Years Survival Rate for {}\".format(YEAR,STYPE))\n",
    "    plt.grid(True)\n",
    "    return plt\n",
    "\n",
    "def loadOHE(df,OHE_LOCATION = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    load enconder to OHE new raw data for prediction\n",
    "    '''\n",
    "    import pickle\n",
    "    with open(OHE_LOCATION + 'encoder.pickle', 'rb') as f:\n",
    "        enc = pickle.load(f) \n",
    "    \n",
    "    #type case object to category\n",
    "    typeCastList = list(df.select_dtypes(include=[object]).columns)\n",
    "    df[typeCastList] = df[typeCastList].astype(\"category\")\n",
    "    OHE_New_Data = enc.transform(df)\n",
    "    \n",
    "    return OHE_New_Data\n",
    "\n",
    "def survivalTable(model,raw_data,interval):\n",
    "    '''\n",
    "    Calculate survival rate in years of interest\n",
    "    Interval are in years.\n",
    "    Raw_data must be in lowercase for values\n",
    "    '''\n",
    "    data = loadOHE(raw_data)\n",
    "    surv = model.predict_survival_function(data)\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    for i, s in enumerate(surv):\n",
    "        x = model.event_times_\n",
    "        y = s\n",
    "    for i in interval:\n",
    "        result = np.where(x > (365.25*(i+1)))[0][0]\n",
    "        dic[i] = y[result]\n",
    "    return dic          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listToDrop = ['NRIC','dob','Has Bills?','Side','Hospital','KKH','NCCS','SGH',\\\n",
    "              'Count_as_DFS','Count_as_OS','Count_as_CSS']\n",
    "\n",
    "clinical = dataSetting(listToDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data of our interest are 5 and 10 years, patient that are new (does not have sufficient records will disturb and mess up our accuracy level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = list([1,5,10])\n",
    "df_dict = ComputeYears(clinical,year_list)\n",
    "\n",
    "# Display shape of data after filtering\n",
    "for i in df_dict:\n",
    "    for s_type in df_dict[i]:\n",
    "        print(\"Year: {}, survival category: {}, size: {}\".format(i,s_type,df_dict[i][s_type].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "YEAR = 5\n",
    "STYPE = \"OS\"\n",
    "k = plotGraph(df_dict,YEAR, STYPE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Years = 5\n",
    "\n",
    "for survival in (\"DFS\", \"OS\", 'CSS'):\n",
    "    time_treatment, survival_prob_treatment = kaplan_meier_estimator(\n",
    "        df_dict['{}_years'.format(YEAR)][survival]['status'],\n",
    "        df_dict['{}_years'.format(YEAR)][survival]['{}_days'.format(survival)])\n",
    "    \n",
    "    plt.step(time_treatment, survival_prob_treatment, where=\"post\",\n",
    "             label=\"Survival Type = {}\".format(survival))\n",
    "\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$ (Days)\")\n",
    "plt.title(\"{} Years Surivial Rate For Each Category\".format(YEAR))\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (30,10)\n",
    "\n",
    "for value in df_dict['{}_years'.format(YEAR)]['OS'][\"TNM_Stage\"].unique():\n",
    "    mask = df_dict['{}_years'.format(YEAR)]['OS'][\"TNM_Stage\"] == value\n",
    "    time_cell, survival_prob_cell = kaplan_meier_estimator(df_dict['{}_years'.format(YEAR)][STYPE]['status'][mask],\n",
    "                                                           df_dict['{}_years'.format(YEAR)][STYPE]['{}_days'.format(STYPE)][mask])\n",
    "    plt.step(time_cell, survival_prob_cell, where=\"post\",\n",
    "             label= '{} (n = {})'.format(value, mask.sum()))\n",
    "\n",
    "plt.ylabel(\"est. probability of survival $\\hat(t)$\")\n",
    "plt.xlabel(\"time $t$\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting population pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survival times are subject to right-censoring, therefore, we need to consider an individual's status\n",
    "in addition to survival time. To be fully compatible with scikit-learn, Status and Survival_in_days\n",
    "need to be stored as a structured array with the first field indicating whether the actual survival time\n",
    "was observed or if was censored, and the second field denoting the observerd survival time,\n",
    "which corresponds to the time of death (if Status == 'dead', $\\delta = 1$) or the last time that\n",
    "person was contacted (if Status == 'alive', $\\delta = 0$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 1\n",
    "STYPE = \"OS\"\n",
    "x_features = list(['tstage','nstage', 'Mstage', 'ER', 'PR', 'Her2', 'size_precise', 'nodespos', 'Age_@_Dx'])\n",
    "y_features = list(['status','OS_days'])\n",
    "\n",
    "X, Y = settingXY(df_dict, x_features, y_features, YEAR, STYPE)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test(X, Y)\n",
    "\n",
    "print(\"X shape: {}\".format(X.shape))\n",
    "print(\"Y shape: {}\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation matrix for CoxPHSurvivalAnalysis - Cox's proportional hazard's model\n",
    "\n",
    "tol is like the p value\n",
    "|1 - (new neg. log-likelihood / old neg. log-likelihood) | < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoxEstimator, Log_Hazard_Ratio = Cox(X,Y)\n",
    "Log_Hazard_Ratio.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log_Hazard_Ratio.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Performance of Survival Models\n",
    "Our test data is usually subject to censoring (only verified records are available, events happening in between is skipped), therefore metrics like root mean squared error or correlation are unsuitable. Instead, we use generalization of the area under the receiver operating characteristic (ROC) curve called Harrell's concordance index or c-index.\n",
    "\n",
    "The interpretation is identical to the traditional area under the ROC curve metric for binary classification:\n",
    "\n",
    "- a value of 0.5 denotes a random model,\n",
    "- a value of 1.0 denotes a perfect model,\n",
    "- a value of 0.0 denotes a perfectly wrong model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "prediction = CoxEstimator.predict(X_test)\n",
    "result = concordance_index_censored(Y_test[\"status\"], Y_test[\"OS_days\"], prediction)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = fit_and_score_features(X_test.values, Y_test)\n",
    "pd.Series(scores, index=X_test.columns).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine features that are useful - Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipe = Pipeline([('encode', OneHotEncoder()),\n",
    "#                  ('select', SelectKBest(fit_and_score_features, k=3)),\n",
    "#                  ('model', CoxPHSurvivalAnalysis(alpha = 1e-6, tol= 1e-6))])\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'select__k': np.arange(1, X.shape[1] + 1)}\n",
    "# gcv = GridSearchCV(pipe, param_grid, return_train_score=True, cv=3, iid=True)\n",
    "# gcv.fit(X, Y)\n",
    "\n",
    "# pd.DataFrame(gcv.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# pipe.set_params(**gcv.best_params_)\n",
    "# pipe.fit(X, Y)\n",
    "\n",
    "# encoder, transformer, final_estimator = [s[1] for s in pipe.steps]\n",
    "# pd.Series(final_estimator.coef_, index=encoder.encoded_columns_[transformer.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "random_state = 20\n",
    "rsf = RandomSurvivalForest(n_estimators=1000,\n",
    "                           min_samples_split=10,\n",
    "                           min_samples_leaf=15,\n",
    "                           max_features=\"sqrt\",\n",
    "                           n_jobs=-1,\n",
    "                           random_state=random_state)\n",
    "rsf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "\n",
    "For prediction, a sample is dropped down each tree in the forest until it reaches a terminal node. Data in each terminal is used to non-parametrically estimate the survival and cumulative hazard function using the Kaplan-Meier and Nelson-Aalen estimator, respectively. In addition, a risk score can be computed that represents the expected number of events for one particular terminal node. The ensemble prediction is simply the average across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_values = X_test.values\n",
    "a = np.empty(X_test.shape[0], dtype=[(\"Age_@_Dx\", float), (\"nodespos\", float)])\n",
    "a[\"Age_@_Dx\"] = X_test_values[:, -1]\n",
    "a[\"nodespos\"] = X_test_values[:, -2]\n",
    "\n",
    "sort_idx = np.argsort(a, order=[\"nodespos\", \"Age_@_Dx\"])\n",
    "\n",
    "X_test_sel = pd.DataFrame(\n",
    "#     X_test_values[np.concatenate((sort_idx[:3], sort_idx[-3:]))],\n",
    "    X_test_values[(sort_idx[:1])],\n",
    "    columns=list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict risk score\n",
    "pd.Series(rsf.predict(X_test_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = rsf.predict_survival_function(X_test_sel)\n",
    "\n",
    "# days\n",
    "# for i, s in enumerate(surv):\n",
    "#     plt.step(rsf.event_times_, s, where=\"post\", label=\"Goh Yakun\")\n",
    "# plt.ylabel(\"Survival probability\")\n",
    "# plt.xlabel(\"Time in days\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# years\n",
    "for i, s in enumerate(surv):\n",
    "    plt.step(rsf.event_times_//365.25, s, where=\"post\", label=\"Goh Yakun\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.xlabel(\"Time (Years)\")\n",
    "plt.title(\"Overall Survival Curve (in Years)\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = rsf.predict_cumulative_hazard_function(X_test_sel)\n",
    "\n",
    "for i, s in enumerate(surv):\n",
    "    plt.step(rsf.event_times_, s, where=\"post\", label=\"Goh Yakun\")\n",
    "plt.ylabel(\"Cumulative hazard\")\n",
    "plt.xlabel(\"Time in days\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = list([0.5,1,2,5,10])\n",
    "raw_data = {'tstage':['tis'],\\\n",
    "            'ER': ['positive'],\\\n",
    "            'PR': ['positive'],\\\n",
    "            'Her2': ['negative'],\\\n",
    "            'size_precise': [1.3],\\\n",
    "            'nodespos': [0],\\\n",
    "            'Age_@_Dx': [21],\\\n",
    "            'nstage': ['n0'],\\\n",
    "            'Mstage': ['m0']}\n",
    "raw_data = pd.DataFrame.from_dict(raw_data)\n",
    "\n",
    "z = survivalTable(rsf,raw_data,interval)\n",
    "print(z.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation-based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = pd.DataFrame({'x':rsf.event_times_,'y':s}, columns = ['x','y'])\n",
    "# tmp.to_csv('C:\\\\Users\\\\LINGXING\\\\Desktop\\\\patient1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# feature_names = X.columns.tolist()\n",
    "# perm = PermutationImportance(rsf, n_iter=15, random_state=random_state)\n",
    "# perm.fit(X_test, Y_test)\n",
    "# eli5.show_weights(perm, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X_features = X_features.dropna(axis = 0, how ='any')  \n",
    "# print(\"Old data frame length:\", len(X_features)) \n",
    "# print(\"New data frame length:\", len(new_X_features)) \n",
    "# print(\"Number of rows with at least 1 NA value: \", (len(X_features)-len(new_X_features))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     'L2_reg': 10.0,\n",
    "#     'batch_norm': True,\n",
    "#     'dropout': 0.4,\n",
    "#     'hidden_layers_sizes': [25, 25],\n",
    "#     'learning_rate': 1e-05,\n",
    "#     'lr_decay': 0.001,\n",
    "#     'momentum': 0.9,\n",
    "#     'n_in': train_data['x'].shape[1],\n",
    "#     'standardize': True\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation: Not enough data per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_features = clinical[['NRIC','Gender', 'c_tstage', 'cNstage', 'cMstage', 'c_Staging', 'tstage',\\\n",
    "#                       'nstage', 'Mstage', 'p_Staging', 'diff', 'TNM_Stage', 'ProgStage_AJCC8',\\\n",
    "#                       'ER', 'PR', 'cerbB2', 'Her2', 'size_precise', 'nodespos', 'Age_@_Dx']]\n",
    "\n",
    "# X_features['COUNTER'] =1  \n",
    "# grouped_data = X_features.groupby(['TNM_Stage','ER', 'PR','Her2']).agg({'COUNTER': ['count']})\n",
    "# grouped_data = grouped_data.reset_index()\n",
    "# df = pd.DataFrame(data=grouped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thought process\n",
    "1. calculate the years of DFS, OS, CSS\n",
    "2. group the data into disease features. calculate the the mean and sd\n",
    "3. caculate the z score and probility area\n",
    "4. split the df into x and y. X = clinical features, Y = years DFS, years CSS, years OS, z score, probility area and SD for modelling category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import zscore\n",
    "# ''' \n",
    "#     calculate avg death for dfs, os, css\n",
    "# ''' \n",
    "\n",
    "# # get all the death records\n",
    "# death_clinical = clinical[clinical['death_age'].notnull()] # shape: (5910, 33)\n",
    "\n",
    "# # get avg for DFS, OS, CSS for those who has deceased\n",
    "# dfs_mean = death_clinical['DFS_years'].mean(axis = 0)\n",
    "# os_mean = death_clinical['OS_years'].mean(axis = 0)\n",
    "# css_mean = death_clinical['CSS_years'].mean(axis = 0)\n",
    "\n",
    "# # get sd for DFS, OS, CSS for those who has deceased\n",
    "# dfs_std = death_clinical['DFS_years'].std(axis = 0,ddof=1)\n",
    "# os_std = death_clinical['OS_years'].std(axis = 0,ddof=1)\n",
    "# css_std = death_clinical['CSS_years'].std(axis = 0,ddof=1)\n",
    "\n",
    "# print(\"dfs_mean: {}, os_mean: {}, css_mean: {}\".format(dfs_mean,os_mean,css_mean))\n",
    "# print(\"dfs_std: {}, os_std: {}, css_std: {}\".format(dfs_std,os_std,css_std))\n",
    "\n",
    "# for x in ['DFS_years', 'OS_years','CSS_years']:\n",
    "#     death_clinical[\"{}_zscore\".format(x)] = death_clinical[[x]].apply(zscore)\n",
    "\n",
    "# death_clinical.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
