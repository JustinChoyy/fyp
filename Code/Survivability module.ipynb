{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_by_index(X,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = pd.read_pickle(\"C:\\\\SMU_v2\\\\clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "to_drop = clinical[clinical['dx_date']==\"NA\"].index\n",
    "clinical = drop_by_index(clinical,to_drop)\n",
    "\n",
    "clinical.drop(columns=['Count_as_DFS','Count_as_OS','Count_as_CSS'],inplace = True)\n",
    "\n",
    "# drop all rows where dates are null\n",
    "clinical.dropna(axis=0, \\\n",
    "                subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date',\\\n",
    "                        'Age_@_Dx','size_precise', 'nodespos',\\\n",
    "                        'cerbB2','ProgStage_AJCC8'],\\\n",
    "                inplace=True)\n",
    "\n",
    "# convert all datetime in dataframe into dateime format for processing\n",
    "clinical[\"Date_for_DFS\"] = pd.to_datetime(clinical[\"Date_for_DFS\"])\n",
    "clinical[\"Date_for_OS\"] = pd.to_datetime(clinical[\"Date_for_OS\"])\n",
    "clinical[\"Date_for_CSS\"] = pd.to_datetime(clinical[\"Date_for_CSS\"])\n",
    "clinical[\"dx_date\"] = pd.to_datetime(clinical[\"dx_date\"])\n",
    "\n",
    "# calculate in days\n",
    "clinical[\"DFS_days\"] = (clinical[\"Date_for_DFS\"] - clinical['dx_date'] )/np.timedelta64(1, 'D')\n",
    "clinical[\"OS_days\"] = (clinical[\"Date_for_OS\"] - clinical['dx_date'] )/np.timedelta64(1, 'D')\n",
    "clinical[\"CSS_days\"] = (clinical[\"Date_for_CSS\"] - clinical['dx_date'] )/np.timedelta64(1, 'D')\n",
    "\n",
    "# alive or dead\n",
    "clinical['status'] = np.where(clinical['death_age'].isnull(), False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "time, survival_prob = kaplan_meier_estimator(clinical['status'], clinical[\"OS_days\"])\n",
    "plt.step(time, survival_prob, where=\"post\")\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for survival in (\"DFS\", \"OS\", 'CSS'):\n",
    "    time_treatment, survival_prob_treatment = kaplan_meier_estimator(\n",
    "        clinical[\"status\"],\n",
    "        clinical[\"{}_days\".format(survival)])\n",
    "    \n",
    "    plt.step(time_treatment, survival_prob_treatment, where=\"post\",\n",
    "             label=\"Survival Type = {}\".format(survival))\n",
    "\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")\n",
    "# plt.grid(True)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (30,10)\n",
    "for value in clinical[\"TNM_Stage\"].unique():\n",
    "    mask = clinical[\"TNM_Stage\"] == value\n",
    "    time_cell, survival_prob_cell = kaplan_meier_estimator(clinical[\"status\"][mask],\n",
    "                                                           clinical[\"OS_days\"][mask])\n",
    "    plt.step(time_cell, survival_prob_cell, where=\"post\",\n",
    "             label=\"%s (n = %d)\" % (value, mask.sum()))\n",
    "\n",
    "plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "plt.xlabel(\"time $t$\")\n",
    "# plt.grid(True)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting population pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survival times are subject to right-censoring, therefore, we need to consider an individual's status\n",
    "in addition to survival time. To be fully compatible with scikit-learn, Status and Survival_in_days\n",
    "need to be stored as a structured array with the first field indicating whether the actual survival time\n",
    "was observed or if was censored, and the second field denoting the observerd survival time,\n",
    "which corresponds to the time of death (if Status == 'dead', $\\delta = 1$) or the last time that\n",
    "person was contacted (if Status == 'alive', $\\delta = 0$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that is meant for Y\n",
    "X = clinical[['Gender', 'c_tstage', 'cNstage', 'cMstage', 'c_Staging', 'tstage',\\\n",
    "                      'nstage', 'Mstage', 'p_Staging', 'diff', 'TNM_Stage', 'ProgStage_AJCC8',\\\n",
    "                      'ER', 'PR', 'cerbB2', 'Her2', 'size_precise', 'nodespos', 'Age_@_Dx']]\n",
    "\n",
    "Y = clinical[['status','OS_days']]\n",
    "\n",
    "\n",
    "# convert to int since some fields fro age_@_dx is null\n",
    "X.loc[:,\"Age_@_Dx\"] = X[\"Age_@_Dx\"].astype(\"int16\")\n",
    "\n",
    "# OHE for probability\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "X = OneHotEncoder().fit_transform(X)\n",
    "\n",
    "# OHE = [i for i in X.columns if not (i in  ['Age_@_Dx', 'size_precise', 'nodespos'])]\n",
    "# X = pd.get_dummies(X,columns=OHE,dummy_na=True)\n",
    "\n",
    "# convert Y to structured array\n",
    "s = Y.dtypes\n",
    "Y = np.array([tuple(x) for x in Y.values], dtype=list(zip(s.index, s)))\n",
    "\n",
    "print(\"X shape: {}\".format(X.shape))\n",
    "print(\"Y shape: {}\".format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation matrix for CoxPHSurvivalAnalysis - Cox's proportional hazard's model\n",
    "\n",
    "tol is like the p value\n",
    "|1 - (new neg. log-likelihood / old neg. log-likelihood) | < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,  test_size=0.33, random_state=42)\n",
    "\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "\n",
    "# since features are highly corelated, reducing alpha values to smaller values allows the learning\n",
    "estimator = CoxPHSurvivalAnalysis(alpha = 1e-4, verbose = 1)\n",
    "estimator.fit(X_train, Y_train)\n",
    "\n",
    "# X.isnull().sum()\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Hazarad Ratio\n",
    "In survival analysis, the hazard ratio (HR) is the ratio of the hazard rates corresponding to the conditions described by two levels of an explanatory variable. For example, in a drug study, the treated population may die at twice the rate per unit time as the control population. The hazard ratio would be 2, indicating higher hazard of death from the treatment. Or in another study, men receiving the same treatment may suffer a certain complication ten times more frequently per unit time than women, giving a hazard ratio of 10. - wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.Series(estimator.coef_, index=X.columns)\n",
    "tmp = tmp.to_frame(\"Log Hazarad Ratio\")\n",
    "tmp = tmp.sort_values(by=['Log Hazarad Ratio'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Performance of Survival Models\n",
    "Our test data is usually subject to censoring (only verified records are available, events happening in between is skipped), therefore metrics like root mean squared error or correlation are unsuitable. Instead, we use generalization of the area under the receiver operating characteristic (ROC) curve called Harrell's concordance index or c-index.\n",
    "\n",
    "The interpretation is identical to the traditional area under the ROC curve metric for binary classification:\n",
    "\n",
    "- a value of 0.5 denotes a random model,\n",
    "- a value of 1.0 denotes a perfect model,\n",
    "- a value of 0.0 denotes a perfectly wrong model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "prediction = estimator.predict(X_test)\n",
    "result = concordance_index_censored(Y_test[\"status\"], Y_test[\"OS_days\"], prediction)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score_features(X, y):\n",
    "    n_features = X.shape[1]\n",
    "    scores = np.empty(n_features)\n",
    "    m = CoxPHSurvivalAnalysis(alpha = 1e-4)\n",
    "    for j in range(n_features):\n",
    "        Xj = X[:, j:j+1]\n",
    "        m.fit(Xj, y)\n",
    "        scores[j] = m.score(Xj, y)\n",
    "    return scores\n",
    "\n",
    "scores = fit_and_score_features(X_test.values, Y_test)\n",
    "pd.Series(scores, index=X_test.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine features that are useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('encode', OneHotEncoder()),\n",
    "                 ('select', SelectKBest(fit_and_score_features, k=3)),\n",
    "                 ('model', CoxPHSurvivalAnalysis(alpha = 1e-4))])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'select__k': np.arange(1, X.shape[1] + 1)}\n",
    "gcv = GridSearchCV(pipe, param_grid, return_train_score=True, cv=3, iid=True)\n",
    "gcv.fit(X, Y)\n",
    "\n",
    "pd.DataFrame(gcv.cv_results_).sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(**gcv.best_params_)\n",
    "pipe.fit(X, Y)\n",
    "\n",
    "encoder, transformer, final_estimator = [s[1] for s in pipe.steps]\n",
    "pd.Series(final_estimator.coef_, index=encoder.encoded_columns_[transformer.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "random_state = 20\n",
    "rsf = RandomSurvivalForest(n_estimators=1000,\n",
    "                           min_samples_split=10,\n",
    "                           min_samples_leaf=15,\n",
    "                           max_features=\"sqrt\",\n",
    "                           n_jobs=-1,\n",
    "                           random_state=random_state)\n",
    "rsf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "perm = PermutationImportance(rsf, n_iter=15, random_state=random_state)\n",
    "perm.fit(X_test, Y_test)\n",
    "eli5.show_weights(perm, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# from sksurv.datasets import load_gbsg2\n",
    "# from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "# X, y = load_gbsg2()\n",
    "\n",
    "# grade_str = X.loc[:, \"tgrade\"].astype(object).values[:, np.newaxis]\n",
    "# grade_num = OrdinalEncoder(categories=[[\"I\", \"II\", \"III\"]]).fit_transform(grade_str)\n",
    "\n",
    "# X_no_grade = X.drop(\"tgrade\", axis=1)\n",
    "# Xt = OneHotEncoder().fit_transform(X_no_grade)\n",
    "# Xt = np.column_stack((Xt.values, grade_num))\n",
    "\n",
    "# feature_names = X_no_grade.columns.tolist() + [\"tgrade\"]\n",
    "\n",
    "# random_state = 20\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     Xt, y, test_size=0.25, random_state=random_state)\n",
    "\n",
    "# Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.empty(X_test.shape[0], dtype=[(\"age\", float), (\"pnodes\", float)])\n",
    "# a[\"age\"] = X_test[:, 0]\n",
    "# a[\"pnodes\"] = X_test[:, 4]\n",
    "\n",
    "# sort_idx = np.argsort(a, order=[\"pnodes\", \"age\"])\n",
    "# X_test_sel = pd.DataFrame(\n",
    "#     X_test[np.concatenate((sort_idx[:3], sort_idx[-3:]))],\n",
    "#     columns=feature_names)\n",
    "\n",
    "# X_test_sel\n",
    "X_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_values = X_test.vaues\n",
    "a = np.empty(X_test.shape[0], dtype=[(\"Age_@_Dx\", float), (\"nodespos\", float)])\n",
    "# a[\"Age_@_Dx\"] = X_test_values[:, 123]\n",
    "# a[\"nodespos\"] = X_test_values[:, 122]\n",
    "\n",
    "# sort_idx = np.argsort(a, order=[\"nodespos\", \"Age_@_Dx\"])\n",
    "# X_test_sel = pd.DataFrame(\n",
    "#     X_test[np.concatenate((sort_idx[:3], sort_idx[-3:]))],\n",
    "#     columns=feature_names)\n",
    "X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = rsf.predict_survival_function(X_test)\n",
    "\n",
    "for i, s in enumerate(surv):\n",
    "    plt.step(rsf.event_times_, s, where=\"post\", label=str(i))\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.xlabel(\"Time in days\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# #Fitting the Logistic Regression Algorithm to the Training Set\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# classifier = LogisticRegression(random_state = 0)\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# #95.8 Acuracy\n",
    "\n",
    "# #Fitting K-NN Algorithm\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# #95.1 Acuracy\n",
    "\n",
    "# #Fitting SVM\n",
    "# from sklearn.svm import SVC\n",
    "# classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "# classifier.fit(X_train, Y_train) \n",
    "# #97.2 Acuracy\n",
    "\n",
    "# #Fitting K-SVM\n",
    "# from sklearn.svm import SVC\n",
    "# classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# #96.5 Acuracy\n",
    "\n",
    "# #Fitting Naive_Bayes\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# classifier = GaussianNB()\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# #91.6 Acuracy\n",
    "\n",
    "# #Fitting Decision Tree Algorithm\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# #95.8 Acuracy\n",
    "\n",
    "# #Fitting Random Forest Classification Algorithm\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "# classifier.fit(X_train, Y_train)\n",
    "# #98.6 Acuracy\n",
    "\n",
    "# #predicting the Test set results\n",
    "# Y_pred = classifier.predict(X_test)\n",
    "\n",
    "# #Creating the confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(Y_test, Y_pred)\n",
    "# c = print(cm[0, 0] + cm[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X_features = X_features.dropna(axis = 0, how ='any')  \n",
    "# print(\"Old data frame length:\", len(X_features)) \n",
    "# print(\"New data frame length:\", len(new_X_features)) \n",
    "# print(\"Number of rows with at least 1 NA value: \", (len(X_features)-len(new_X_features))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsToIter = ['Gender', 'c_tstage', 'cNstage', 'cMstage', 'c_Staging', 'tstage',\n",
    "                 'nstage', 'Mstage', 'p_Staging', 'diff', 'TNM_Stage', 'ProgStage_AJCC8',\n",
    "                 'ER', 'PR', 'cerbB2', 'Her2']\n",
    "\n",
    "uniques = {col: X_features[col].unique().tolist() for col in columnsToIter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'L2_reg': 10.0,\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.4,\n",
    "    'hidden_layers_sizes': [25, 25],\n",
    "    'learning_rate': 1e-05,\n",
    "    'lr_decay': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'n_in': train_data['x'].shape[1],\n",
    "    'standardize': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation: Not enough data per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_features = clinical[['NRIC','Gender', 'c_tstage', 'cNstage', 'cMstage', 'c_Staging', 'tstage',\\\n",
    "#                       'nstage', 'Mstage', 'p_Staging', 'diff', 'TNM_Stage', 'ProgStage_AJCC8',\\\n",
    "#                       'ER', 'PR', 'cerbB2', 'Her2', 'size_precise', 'nodespos', 'Age_@_Dx']]\n",
    "\n",
    "# X_features['COUNTER'] =1  \n",
    "# grouped_data = X_features.groupby(['TNM_Stage','ER', 'PR','Her2']).agg({'COUNTER': ['count']})\n",
    "# grouped_data = grouped_data.reset_index()\n",
    "# df = pd.DataFrame(data=grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thought process\n",
    "1. calculate the years of DFS, OS, CSS\n",
    "2. group the data into disease features. calculate the the mean and sd\n",
    "3. caculate the z score and probility area\n",
    "4. split the df into x and y. X = clinical features, Y = years DFS, years CSS, years OS, z score, probility area and SD for modelling category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import zscore\n",
    "# ''' \n",
    "#     calculate avg death for dfs, os, css\n",
    "# ''' \n",
    "\n",
    "# # get all the death records\n",
    "# death_clinical = clinical[clinical['death_age'].notnull()] # shape: (5910, 33)\n",
    "\n",
    "# # get avg for DFS, OS, CSS for those who has deceased\n",
    "# dfs_mean = death_clinical['DFS_years'].mean(axis = 0)\n",
    "# os_mean = death_clinical['OS_years'].mean(axis = 0)\n",
    "# css_mean = death_clinical['CSS_years'].mean(axis = 0)\n",
    "\n",
    "# # get sd for DFS, OS, CSS for those who has deceased\n",
    "# dfs_std = death_clinical['DFS_years'].std(axis = 0,ddof=1)\n",
    "# os_std = death_clinical['OS_years'].std(axis = 0,ddof=1)\n",
    "# css_std = death_clinical['CSS_years'].std(axis = 0,ddof=1)\n",
    "\n",
    "# print(\"dfs_mean: {}, os_mean: {}, css_mean: {}\".format(dfs_mean,os_mean,css_mean))\n",
    "# print(\"dfs_std: {}, os_std: {}, css_std: {}\".format(dfs_std,os_std,css_std))\n",
    "\n",
    "# for x in ['DFS_years', 'OS_years','CSS_years']:\n",
    "#     death_clinical[\"{}_zscore\".format(x)] = death_clinical[[x]].apply(zscore)\n",
    "\n",
    "# death_clinical.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
