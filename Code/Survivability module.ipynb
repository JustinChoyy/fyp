{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LINGXING\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import pickle\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sklearn.externals import joblib \n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_by_index(X,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    return(X)\n",
    "\n",
    "def dataSetting(dropCol,FILE_FOLDER = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    function to read the pkl from from datasource\n",
    "        1. Remove dx_date that is NULL.\n",
    "        2. Drop all rows where crucial fields for X_features are NULL.\n",
    "        3. Convert Date columns into datetime format\n",
    "        4. Derive OS, CSS, DFS days based on dx_date\n",
    "        5. Create status column to indicate if the patient is dead or alive base on if death_age exists\n",
    "    '''\n",
    "    df = pd.read_pickle(FILE_FOLDER + \"clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "    to_drop = df[df['dx_date']==\"NA\"].index\n",
    "    df = drop_by_index(df,to_drop)\n",
    "\n",
    "    df.drop(columns=dropCol,inplace = True)\n",
    "\n",
    "    # drop all rows where dates are null\n",
    "    df.dropna(axis=0,\\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date','Age_@_Dx'],\\\n",
    "                    inplace=True)\n",
    "    \n",
    "    # convert all datetime in dataframe into dateime format for processing\n",
    "    df[\"Date_for_DFS\"] = pd.to_datetime(df[\"Date_for_DFS\"])\n",
    "    df[\"Date_for_OS\"] = pd.to_datetime(df[\"Date_for_OS\"])\n",
    "    df[\"Date_for_CSS\"] = pd.to_datetime(df[\"Date_for_CSS\"])\n",
    "    df[\"dx_date\"] = pd.to_datetime(df[\"dx_date\"])\n",
    "    df['last_seen']= pd.to_datetime(df[\"dx_date\"])\n",
    "    df['dob']= pd.to_datetime(df[\"dx_date\"])\n",
    "\n",
    "    # calculate in days\n",
    "    df[\"DFS_days\"] = (df[\"Date_for_DFS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"OS_days\"] = (df[\"Date_for_OS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"CSS_days\"] = (df[\"Date_for_CSS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "\n",
    "    # alive or dead\n",
    "#     df['status'] = np.where(df['Count_as_OS'] == \"dead\", False, True)\n",
    "    df['status'] = np.where(df['death_age'].isnull(), False, True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def ComputeYears(df, Year_list):\n",
    "    '''\n",
    "    Create a list to contain df for different years of survival\n",
    "    The df will filter those patient that has deceased or days of survival longer than the defined years.\n",
    "    '''\n",
    "\n",
    "    df_dict = {}\n",
    "\n",
    "    for i in Year_list:\n",
    "        tmp = {}\n",
    "        for x in list([\"DFS\", \"CSS\", \"OS\"]):\n",
    "            df['{}_{}_years'.format(x, i)] = np.where(\n",
    "                                                      np.logical_or(df['death_age'] > 0,\\\n",
    "                                                      df['{}_days'.format(x)]/(365.25*i) >= i),\\\n",
    "                                                      True,False)\n",
    "            tmp[x] = df[df['{}_{}_years'.format(x, i)] == True]\n",
    "        df_dict['{}_years'.format(i)] = tmp\n",
    "    return df_dict\n",
    "\n",
    "def dropSubGroup(df,colToDropSubGroup,subgroups, notDropSubgroups):\n",
    "    '''\n",
    "    drop patient records that contains T,N.M subgroups ('a','b','c')\n",
    "    \n",
    "    notDropSubGroups is a list of values that we want to be cautious and not drop during processing\n",
    "    '''\n",
    "    index_list = set()\n",
    "    index_not_drop = set()\n",
    "    for col in colToDropSubGroup:\n",
    "        for subgroup in subgroups:\n",
    "            index_list.update(list(df[df['{}'.format(col)].str.contains(subgroup)].index))\n",
    "        for notDropgroup in notDropSubgroups:\n",
    "            index_not_drop.update(list(df[df['{}'.format(col)].str.contains(notDropgroup)].index))\n",
    "    \n",
    "    # in-depth filtering\n",
    "    tmp = set()\n",
    "    for i in index_not_drop:\n",
    "        chance = 1\n",
    "        for col in colToDropSubGroup:\n",
    "            for subgroup in subgroups: \n",
    "                for notDropgroup in notDropSubgroups:\n",
    "                    if subgroup in df[col][i] and not (notDropgroup in df[col][i]):\n",
    "                        chance = 0\n",
    "        if chance == 1:\n",
    "            tmp.add(i)\n",
    "    \n",
    "    cfm_index_list = list(index_list.difference(tmp))\n",
    "    \n",
    "    return drop_by_index(df,cfm_index_list)\n",
    "\n",
    "def train_test(X, Y, test_size = 0.33, random_state = 42):\n",
    "    '''\n",
    "    Splitting the dataset into the Training set and Test set\n",
    "    '''\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,  test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def Cox(X_train,Y_train,alpha = 1e-4, verbose = 0):\n",
    "        \n",
    "    # since features are highly corelated, reducing alpha values to smaller values allows the learning\n",
    "    model = CoxPHSurvivalAnalysis(alpha = alpha, verbose = verbose)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_and_score_features(X, y):\n",
    "    '''\n",
    "    Based on the Cox model, rank the scores of each feature to understand which X features plays the key role in\n",
    "    modelling\n",
    "    '''\n",
    "    n_features = X.shape[1]\n",
    "    scores = np.empty(n_features)\n",
    "    m = CoxPHSurvivalAnalysis(alpha = 1e-4)\n",
    "    for j in range(n_features):\n",
    "        Xj = X[:, j:j+1]\n",
    "        m.fit(Xj, y)\n",
    "        scores[j] = m.score(Xj, y)\n",
    "    return scores\n",
    "\n",
    "def plotGraph(df, YEAR, STYPE, UNITS = 0, ):\n",
    "    '''\n",
    "    function to plot the graph\n",
    "    UNITS: {0: days, 1: years}\n",
    "    '''\n",
    "    if UNITS == 1:\n",
    "        unit = \"Years\"\n",
    "    else:\n",
    "        unit = \"Days\"\n",
    "        \n",
    "    time, survival_prob = kaplan_meier_estimator(df['{}_years'.format(YEAR)][STYPE]['status'], \n",
    "                                                 df['{}_years'.format(YEAR)][STYPE]['{}_days'.format(STYPE)])\n",
    "    \n",
    "    if UNITS == 1:\n",
    "        time = time/365.25\n",
    "    plt.step(time, survival_prob, where=\"post\")\n",
    "    \n",
    "    plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "    plt.xlabel(\"time $t$ ({})\".format(unit))\n",
    "    plt.title(\"{} Years Survival Rate for {}\".format(YEAR,STYPE))\n",
    "    plt.grid(True)\n",
    "    return plt\n",
    "\n",
    "def FeaturesPriority(df,x_features,y_features):\n",
    "    # change unknown to nx\n",
    "    df['cNstage'].replace(to_replace =\"unknown\", \n",
    "                                       value =\"nx\", inplace = True) \n",
    "    df['nstage'].replace(to_replace =\"unknown\", \n",
    "                                   value =\"nx\", inplace = True) \n",
    "\n",
    "    # to prevent changing of the orginial df\n",
    "    working_df = df.copy()\n",
    "    \n",
    "    t_score = {\n",
    "                \"t4d\": 1, 't4c':2,'t4b':3,'t4a':4,'t4':5,\\\n",
    "                't3': 6,'t2':7,\\\n",
    "                't1c': 8,'t1b':9,'t1a': 10,'t1mic':11,'t1': 12,\\\n",
    "                't0':13,'tis':14,'tx': 15\n",
    "               }\n",
    "    \n",
    "    m_score = {'m1a': 1, 'm1': 2, 'm0': 3, 'mx': 4}\n",
    "    \n",
    "    n_score = {\n",
    "                'n3c':1,'n3b':2,'n3a':3,'n3':4,\\\n",
    "                'n2b':5,'n2a':6,'n2':7,\\\n",
    "                'n1c':8,'n1b':9,'n1a':10,'n1mic':11,'n1':12,\\\n",
    "                'n0 (i+)':13,'n0':14,'nx':15\n",
    "                }\n",
    "    \n",
    "    working_df['t_1'] = working_df['tstage'].map(t_score)\n",
    "    working_df['t_2'] = working_df['c_tstage'].map(t_score)\n",
    "    \n",
    "    working_df['m_1'] = working_df['Mstage'].map(m_score)\n",
    "    working_df['m_2'] = working_df['cMstage'].map(m_score)\n",
    "    \n",
    "    working_df['n_1'] = working_df['cNstage'].map(n_score)\n",
    "    working_df['n_2'] = working_df['nstage'].map(n_score)\n",
    "    \n",
    "    # convert as new columns are categorical\n",
    "    for x in ['t_1','t_2','m_1','m_2','n_1','n_2']:\n",
    "        working_df.loc[:,x] = working_df[x].astype(\"int16\")\n",
    "    \n",
    "    working_df.loc[working_df['t_1'] > working_df['t_2'] ,'tstage_tmp'] = working_df['c_tstage']\n",
    "    working_df.loc[working_df['t_1'] <= working_df['t_2'] ,'tstage_tmp'] = working_df['tstage']\n",
    "    \n",
    "    working_df.loc[working_df['m_1'] > working_df['m_2'] ,'Mstage_tmp'] = working_df['cMstage']\n",
    "    working_df.loc[working_df['m_1'] <= working_df['m_2'] ,'Mstage_tmp'] = working_df['Mstage']\n",
    "    \n",
    "    working_df.loc[working_df['n_1'] > working_df['n_2'] ,'nstage_tmp'] = working_df['nstage']\n",
    "    working_df.loc[working_df['n_1'] <= working_df['n_2'] ,'nstage_tmp'] = working_df['cNstage']\n",
    "    \n",
    "    working_df.drop(columns = [\"Mstage\",'nstage','tstage'], inplace = True)\n",
    "    working_df.rename(columns={'Mstage_tmp':'Mstage', 'nstage_tmp':'nstage','tstage_tmp':'tstage'}, inplace=True)\n",
    "    \n",
    "    working_df.loc[:,'tstage'] = working_df['tstage'].astype(\"category\")\n",
    "    working_df.loc[:,'nstage'] = working_df['nstage'].astype(\"category\")\n",
    "    working_df.loc[:,'Mstage'] = working_df['Mstage'].astype(\"category\")\n",
    "    \n",
    "    x_features = [e for e in x_features if e not in ['c_tstage','cNstage','cMstage']]\n",
    "    return working_df[x_features+y_features]\n",
    "\n",
    "def settingXY(df, X_features, Y_features, OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\", name=\"\"):\n",
    "    '''\n",
    "    This function returns the X and Y features need for model training\n",
    "        - The function also generates one pkl that contains the One Hot Encoder for new raw data \n",
    "    \n",
    "    X_features = features to use for X\n",
    "    Y_features = features to use for Y \n",
    "    YEAR = years of patient record interested\n",
    "    SYTPE = survival type (OS, DFS, CSS)\n",
    "    OHE_LOCATION = location to store the pkl file\n",
    "    '''\n",
    "    for i in  X_features:\n",
    "        if not (i in ['nodespos','Age_@_Dx','size_precise']):\n",
    "            df.loc[:,i] = df[i].astype(\"category\")\n",
    "        else:\n",
    "            df.loc[:,i] = df[i].astype(\"float32\")\n",
    "    \n",
    "    X = df[X_features]\n",
    "    Y = df[Y_features]\n",
    "\n",
    "    # Save enconder so that we can OHE new data\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    \n",
    "    # OHE for probability\n",
    "    X = enc.transform(X)\n",
    "    with open(OHE_LOCATION + name + '_encoder.pickle', 'wb') as f:\n",
    "        pickle.dump(enc, f) \n",
    "                  \n",
    "    # convert Y to structured array\n",
    "    s = Y.dtypes\n",
    "    Y = np.array([tuple(x) for x in Y.values], dtype=list(zip(s.index, s)))\n",
    "   \n",
    "    return X, Y\n",
    "def layeredData(df, group_dict,y_features, YEAR, STYPE):\n",
    "    \n",
    "    '''\n",
    "        this function generates the dataframe required for specific groups we hope to analyze\n",
    "        there are total 3 different groups but group 3 consist of multiple subgroups which leads a total of 5\n",
    "        dataframe.\n",
    "        Group 1: patient with stage 4 cancer\n",
    "        Group 2: patient which unknown records or at initial diagnosis stage\n",
    "        Group 3: make up of patient that does not belong to the groups above\n",
    "    '''\n",
    "    model_data_dict = {}\n",
    "    TO_USE = df['{}_years'.format(YEAR)][STYPE]\n",
    "    \n",
    "    print(\"Overall initial size: {} \\n\".format(TO_USE.shape[0]))\n",
    "        \n",
    "    for key,value in group_dict.items():\n",
    "        TO_USE_COPY = TO_USE.copy()\n",
    "\n",
    "        tmp = {}\n",
    "        \n",
    "        waves = value['wave']\n",
    "    \n",
    "        if key != \"group 3\":\n",
    "            # for group 1 and group 2 select rows that contains either stage 4/non invasive in Stage\n",
    "            TO_USE_COPY = TO_USE_COPY.loc[TO_USE_COPY['Stage'] == group_dict[key]['stage'][0]]\n",
    "        else:\n",
    "            # for group 3 do not select rows that contains either stage 4 or non invasive in c_Staging or p_Staging\n",
    "            stage = np.logical_and(TO_USE_COPY['Stage'] != group_dict[key]['stage'][0],\\\n",
    "                                    TO_USE_COPY['Stage'] != group_dict[key]['stage'][1])\n",
    "            \n",
    "            TO_USE_COPY = TO_USE_COPY.loc[stage]\n",
    "            \n",
    "        print(\"{} data size: {}\".format(key,len(TO_USE_COPY)))\n",
    "        \n",
    "        for wave in waves:\n",
    "            TO_USE_COPY2 = TO_USE_COPY.copy()\n",
    "            TO_USE_COPY2 = TO_USE_COPY2[waves[wave] + y_features]\n",
    "            \n",
    "            len_before = len(TO_USE_COPY2)\n",
    "            print(\"\\t{} data size before dropping nan: {}\".format(wave,len_before))\n",
    "            \n",
    "            TO_USE_COPY2.dropna(axis=0,subset=waves[wave]+ y_features, inplace=True)\n",
    "            TO_USE_COPY2.reset_index(drop=True)\n",
    "\n",
    "            len_after = len(TO_USE_COPY2)\n",
    "            print(\"\\t\\t after dropping nan: {}\".format(len_after))\n",
    "            \n",
    "            X, Y = settingXY(TO_USE_COPY2, waves[wave], y_features,name= \"{}_{}\".format(key,wave))   \n",
    "            \n",
    "            TO_USE_COPY2.to_pickle(\"C:\\\\SMU_v2\\\\Layered Folder\\\\{}_{}.pkl\".format(key,wave))\n",
    "\n",
    "            tmp[wave] = {\n",
    "                            \"X\": X,\\\n",
    "                            \"Y\":Y      \n",
    "                        }    \n",
    "    \n",
    "        model_data_dict[key] = tmp\n",
    "        \n",
    "        \n",
    "    return model_data_dict\n",
    "\n",
    "def loadOHE(df,OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\", name=\"\"):\n",
    "    '''\n",
    "    load enconder to OHE new raw data for prediction\n",
    "    '''\n",
    "    with open( \"{}{}{}\".format(OHE_LOCATION, name, '_encoder.pickle'), 'rb') as f:\n",
    "        enc = pickle.load(f) \n",
    "    \n",
    "    #type case object to category\n",
    "    typeCastList = list(df.select_dtypes(include=[object]).columns)\n",
    "    df[typeCastList] = df[typeCastList].astype(\"category\")\n",
    "    OHE_New_Data = enc.transform(df)\n",
    "    \n",
    "    return OHE_New_Data\n",
    "\n",
    "def survivalTable(modelName, raw_data,OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\",interval = list([0.5,1,2,5,10])):\n",
    "    '''\n",
    "    Calculate survival rate in years of interest\n",
    "    '''\n",
    "\n",
    "    for k,v in raw_data.items():\n",
    "        if str(v[0]).isalpha():\n",
    "            raw_data[k] = v[0].lower()\n",
    "        \n",
    "    raw_data = pd.DataFrame.from_dict(raw_data)\n",
    "    \n",
    "    model = joblib.load('Model_folder\\\\{}.pkl'.format(modelName))\n",
    "\n",
    "    with open( \"{}{}{}\".format(OHE_LOCATION, modelName[:-4], '_encoder.pickle'), 'rb') as f:\n",
    "            enc = pickle.load(f) \n",
    "        \n",
    "    #type case object to category\n",
    "    typeCastList = list(raw_data.select_dtypes(include=[object]).columns)\n",
    "    raw_data[typeCastList] = raw_data[typeCastList].astype(\"category\")\n",
    "    data = enc.transform(raw_data)\n",
    "\n",
    "    surv = model.predict_survival_function(data)\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    for i, s in enumerate(surv):\n",
    "        x = model.event_times_\n",
    "        y = s\n",
    "    graphaxis = pd.DataFrame({'x':x,'y':y}, columns = ['x','y'])\n",
    "    for i in interval:\n",
    "        result = np.where(x > (365.25*(i+1)))[0][0]\n",
    "        dic[i] = y[result]\n",
    "\n",
    "    return dic,graphaxis\n",
    "\n",
    "def ann_structure(input_shape,output_units):\n",
    "    \"\"\"\n",
    "    function to declare ANN structure. just for code cleaniness\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(32, input_shape=(input_shape,)))         # input layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    \n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                        beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                                        moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                        beta_constraint=None, gamma_constraint=None)\n",
    ") \n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(output_units, activation=tf.nn.leaky_relu))   # one output layer with 1 outputs\n",
    "    return model\n",
    "def display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse):\n",
    "    graph = pd.DataFrame(np.arange(0,2,.01),columns=[\"Percentage\"])\n",
    "    graph[\"viz\"] = graph.applymap(lambda x: get_percentage(predictions_scaled_reverse,y_test_scaled_reverse,x))\n",
    "    show = graph.plot.area(x=\"Percentage\")\n",
    "    show.set_title(\"Model performance ({})\".format(scope))\n",
    "    show.set_xlabel(\"Percentage Difference from Ground Truth\")\n",
    "    show.set_ylabel(\"Percentage of all our predictions\")\n",
    "    show.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    show.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_@_Dx</th>\n",
       "      <th>diff=grade 2</th>\n",
       "      <th>diff=grade 3</th>\n",
       "      <th>diff=unknown</th>\n",
       "      <th>ER=negative</th>\n",
       "      <th>ER=positive</th>\n",
       "      <th>ER=unknown</th>\n",
       "      <th>PR=negative</th>\n",
       "      <th>PR=positive</th>\n",
       "      <th>PR=unknown</th>\n",
       "      <th>Her2=negative</th>\n",
       "      <th>Her2=not done</th>\n",
       "      <th>Her2=positive</th>\n",
       "      <th>Her2=unknown</th>\n",
       "      <th>Stage=stage 0</th>\n",
       "      <th>Stage=stage 1</th>\n",
       "      <th>Stage=stage 1a</th>\n",
       "      <th>Stage=stage 1b</th>\n",
       "      <th>Stage=stage 2</th>\n",
       "      <th>Stage=stage 2a</th>\n",
       "      <th>Stage=stage 2b</th>\n",
       "      <th>Stage=stage 3</th>\n",
       "      <th>Stage=stage 3a</th>\n",
       "      <th>Stage=stage 3b</th>\n",
       "      <th>Stage=stage 3c</th>\n",
       "      <th>Stage=stage 4</th>\n",
       "      <th>Stage=unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age_@_Dx  diff=grade 2  diff=grade 3  diff=unknown  ER=negative  \\\n",
       "2      46.0           0.0           0.0           0.0          0.0   \n",
       "\n",
       "   ER=positive  ER=unknown  PR=negative  PR=positive  PR=unknown  \\\n",
       "2          1.0         0.0          0.0          1.0         0.0   \n",
       "\n",
       "   Her2=negative  Her2=not done  Her2=positive  Her2=unknown  Stage=stage 0  \\\n",
       "2            1.0            0.0            0.0           0.0            0.0   \n",
       "\n",
       "   Stage=stage 1  Stage=stage 1a  Stage=stage 1b  Stage=stage 2  \\\n",
       "2            0.0             1.0             0.0            0.0   \n",
       "\n",
       "   Stage=stage 2a  Stage=stage 2b  Stage=stage 3  Stage=stage 3a  \\\n",
       "2             0.0             0.0            0.0             0.0   \n",
       "\n",
       "   Stage=stage 3b  Stage=stage 3c  Stage=stage 4  Stage=unknown  \n",
       "2             0.0             0.0            0.0            0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:\\\\SMU_v2\\\\Layered Folder\\\\'\n",
    "\n",
    "y_features = list(['status','OS_days'])\n",
    "\n",
    "if len(fnmatch.filter(os.listdir(path), '*.pkl')) > 0:\n",
    "    \n",
    "    model_data_dict = {}\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.pkl' in file:\n",
    "                group = str(file).split(\"_\")[0]\n",
    "                wave = str(file).split(\"_\")[1].split(\".\")[0]\n",
    "                tmp = pd.read_pickle(path+file)\n",
    "\n",
    "                x_features = [i for i in tmp.columns if i not in y_features]\n",
    "                \n",
    "                X, Y = settingXY(tmp, x_features, y_features,name= \"{}_{}\".format(group,wave))  \n",
    "                \n",
    "                if not (group in model_data_dict): \n",
    "                    model_data_dict[group] = {wave : { \"X\": X,\\\n",
    "                                                       \"Y\":Y\n",
    "                                                      }}\n",
    "                else:\n",
    "                    model_data_dict[group].update({wave : { \"X\": X,\\\n",
    "                                                            \"Y\":Y\n",
    "                                                          }} )\n",
    "                del X\n",
    "                del Y\n",
    "                del tmp\n",
    "    print(\"Data loaded!\")                \n",
    "else:\n",
    "    # Data Processing\n",
    "    listToDrop = ['NRIC','dob','Has Bills?','Side','Hospital','KKH','NCCS','SGH','END_OF_ENTRY']\n",
    "    clinical = dataSetting(listToDrop)\n",
    "    print(clinical.shape)\n",
    "\n",
    "    # Data of our interest are 5 and 10 years, patient that are new \n",
    "    # (does not have sufficient records will disturb and mess up our accuracy level\n",
    "    # only return data that has longer timeframe than the given interval\n",
    "\n",
    "    year_list = list([1,5,10])\n",
    "    df_dict = ComputeYears(clinical,year_list)\n",
    "\n",
    "    # Display shape of data after filtering\n",
    "    for i in df_dict: \n",
    "        for s_type in df_dict[i]:\n",
    "            print(\"Year: {}, survival category: {}, size: {}\".format(i,s_type,df_dict[i][s_type].shape[0]))\n",
    "            \n",
    "    YEAR = 1\n",
    "    STYPE = \"OS\"\n",
    "\n",
    "    group_dict = { \n",
    "                    \"group 1\": {\n",
    "                                 \"stage\": ['stage 4'],\\\n",
    "                                 'wave': {\n",
    "                                             \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                             \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                         'T (no subgroup)', 'N (no subgroup)'],\\\n",
    "                                             \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N'],\\\n",
    "                                             \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise', 'nodespos']\n",
    "                                         }\n",
    "                               },\\\n",
    "                    \"group 2\": {\n",
    "                                 'stage': ['dcis/lcis non-invasive'],\\\n",
    "                                 'wave': {\n",
    "                                             \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Size'],\\\n",
    "                                             \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','size_precise']\n",
    "                                         }\n",
    "                               },\\\n",
    "                    \"group 3\": {\n",
    "                                 \"stage\": ['stage 4','dcis/lcis non-invasive'],\\\n",
    "                                 'wave': {\n",
    "                                             \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                             \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                         'T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)'],\\\n",
    "                                             \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N', 'M'],\\\n",
    "                                             \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise',\\\n",
    "                                                         'nodespos','M'],\\\n",
    "                                             'layer 5':['T','N', 'M', 'ER', 'PR', 'Her2',\\\n",
    "                                                        'size_precise', 'nodespos', 'Age_@_Dx']\n",
    "                                         }\n",
    "                               },\n",
    "                    }\n",
    "    model_data_dict = layeredData(df_dict, group_dict,y_features,YEAR, STYPE)\n",
    "    print(\"Processing Done!\")\n",
    "model_data_dict['group 3']['layer 1']['X'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation(X, Y, label, cox_alpha=1e-4, n_estimators = 1000, random_state = 20, file_location = \"Model_folder\\\\\"):\n",
    "    \n",
    "    result = {}\n",
    "    X_train, X_test, Y_train, Y_test = train_test(X, Y)\n",
    "    print(\"\\t\\t X_train:{}, X_test:{}\".format(X_train.shape,X_test.shape))\n",
    "    \n",
    "    rsf = RandomSurvivalForest(n_estimators= n_estimators ,\n",
    "                               max_depth=None,\n",
    "                               max_leaf_nodes=None, \n",
    "                               bootstrap=True,\n",
    "                               oob_score=False,\n",
    "                               min_samples_split=10,\n",
    "                               min_samples_leaf=15,\n",
    "                               max_features=\"sqrt\",\n",
    "                               n_jobs=-1,\n",
    "                               random_state=random_state)\n",
    "    \n",
    "    model_rsf = rsf.fit(X_train, Y_train)\n",
    "    result[\"rsf\".format(label)] = model_rsf.score(X_test, Y_test)\n",
    "    \n",
    "    # save the model to disk\n",
    "    filename = '{}{}_rsf.pkl'.format(file_location,label)\n",
    "    joblib.dump(model_rsf, filename)\n",
    "    \n",
    "    del model_rsf\n",
    "    gc.collect()\n",
    "    \n",
    "    model_cox = Cox(X_train,Y_train, cox_alpha)\n",
    "    prediction = model_cox.predict(X_test)\n",
    "    result[\"cox\".format(label)] = concordance_index_censored(Y_test[\"status\"],\\\n",
    "                                                                          Y_test[\"OS_days\"], prediction)[0]\n",
    "    filename = '{}{}_cox.pkl'.format(file_location,label)\n",
    "    joblib.dump(model_cox, filename) \n",
    "    \n",
    "    del model_cox\n",
    "    gc.collect()\n",
    "    \n",
    "    for k,v in result.items():\n",
    "        print(\"\\t\\t{}:{}\".format(k,v))  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listToDrop = ['NRIC','dob','Has Bills?','Side','Hospital','KKH','NCCS','SGH',\\\n",
    "#               'Count_as_DFS','Count_as_CSS']\n",
    "\n",
    "# clinical = dataSetting(listToDrop)\n",
    "# year_list = list([1,5,10])\n",
    "# df_dict = ComputeYears(clinical,year_list)\n",
    "\n",
    "# x_features = list(['T','N', 'M', 'ER', 'PR', 'Her2', 'size_precise', 'nodespos', 'Age_@_Dx'])\n",
    "# y_features = list(['status','OS_days'])\n",
    "# tmp = df_dict['1_years'][\"OS\"]\n",
    "\n",
    "# tmp.dropna(axis=0,\\\n",
    "#             subset=x_features,\\\n",
    "#             inplace=True)\n",
    "# X, Y = settingXY(tmp, x_features, y_features)\n",
    "\n",
    "# model_creation(X, Y, \"group 3_layer 5\", cox_alpha=1e-4, n_estimators = 1000, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 1:\n",
      "\t layer 1:\n",
      "\t\t X_train:(1359, 27), X_test:(670, 27)\n",
      "\t\trsf:0.6313131313131313\n",
      "\t\tcox:0.6324765415185222\n",
      "\t layer 2:\n",
      "\t\t X_train:(1359, 25), X_test:(670, 25)\n",
      "\t\trsf:0.648603907753531\n",
      "\t\tcox:0.6497455719737744\n",
      "\t layer 3:\n",
      "\t\t X_train:(1359, 42), X_test:(670, 42)\n",
      "\t\trsf:0.650446879994781\n",
      "\t\tcox:0.650060888758413\n",
      "\t layer 4:\n",
      "\t\t X_train:(306, 16), X_test:(151, 16)\n",
      "\t\trsf:0.6971401781528364\n",
      "\t\tcox:0.6901078293483357\n",
      "group 2:\n",
      "\t layer 1:\n",
      "\t\t X_train:(1313, 20), X_test:(648, 20)\n",
      "\t\trsf:0.6639286469717267\n",
      "\t\tcox:0.6738701262074315\n",
      "\t layer 2:\n",
      "\t\t X_train:(1333, 15), X_test:(657, 15)\n",
      "\t\trsf:0.6638031208499336\n",
      "\t\tcox:0.6493193891102258\n",
      "group 3:\n",
      "\t layer 1:\n",
      "\t\t X_train:(12699, 27), X_test:(6255, 27)\n",
      "\t\trsf:0.738791538076107\n",
      "\t\tcox:0.7326479878165673\n",
      "\t layer 2:\n",
      "\t\t X_train:(12699, 27), X_test:(6255, 27)\n",
      "\t\trsf:0.7410006491577815\n",
      "\t\tcox:0.7365595953994541\n",
      "\t layer 3:\n",
      "\t\t X_train:(12699, 45), X_test:(6255, 45)\n",
      "\t\trsf:0.7382000763521447\n",
      "\t\tcox:0.737469876692932\n",
      "\t layer 4:\n",
      "\t\t X_train:(8616, 19), X_test:(4244, 19)\n",
      "\t\trsf:0.7427278875378437\n",
      "\t\tcox:0.7127040986832088\n",
      "\t layer 5:\n",
      "\t\t X_train:(8616, 44), X_test:(4244, 44)\n",
      "\t\trsf:0.7467824239320228\n",
      "\t\tcox:0.7334676853226919\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "for group,wave_dict in model_data_dict.items():\n",
    "    print(\"{}:\".format(group))\n",
    "    tmp = {}\n",
    "    sub_data_dict = {}\n",
    "    \n",
    "    if group == \"group 3\":\n",
    "        cox_alpha = 1e-2\n",
    "        n_estimators = 400\n",
    "    else:\n",
    "        cox_alpha = 1e-4\n",
    "        n_estimators = 1000\n",
    "    \n",
    "    for wave in wave_dict:\n",
    "        print(\"\\t {}:\".format(wave))\n",
    "        X = wave_dict[wave]['X']\n",
    "        Y = wave_dict[wave]['Y']\n",
    "\n",
    "        result = model_creation(X,Y, \"{}_{}\".format(group,wave), cox_alpha, n_estimators )\n",
    "\n",
    "#             #### ANN\n",
    "#             Y = pd.DataFrame(Y.tolist(), columns = ['Status','OS_days'])\n",
    "#             Y['Status'] = np.where(Y['Status']== True, 1, 0)\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#             model = ann_structure(X.shape[1],Y.shape[1])\n",
    "#             model.compile(optimizer=tf.keras.optimizers.Adam(),\\\n",
    "#                                       loss='mean_squared_error')\n",
    "#             # Run the stochastic gradient descent for specified epochs\n",
    "#             epochs = 100\n",
    "#             filepath=\"weights.best.{}.h5\".format(wave)\n",
    "#             callbacks_list = []\n",
    "#             callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "#             # callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "#             model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))\n",
    "\n",
    "#             pred = pd.DataFrame(model.predict(X_test))\n",
    "#             o+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.998539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.998539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.998492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.998354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.998354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77.0</td>\n",
       "      <td>0.997593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81.0</td>\n",
       "      <td>0.997593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.997503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.997483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.997343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.0</td>\n",
       "      <td>0.997290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.997290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>112.0</td>\n",
       "      <td>0.997187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.996001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.994089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>143.0</td>\n",
       "      <td>0.993277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.990835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>145.0</td>\n",
       "      <td>0.984475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>162.0</td>\n",
       "      <td>0.983783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>173.0</td>\n",
       "      <td>0.981217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>184.0</td>\n",
       "      <td>0.976359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>206.0</td>\n",
       "      <td>0.976325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>230.0</td>\n",
       "      <td>0.976325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0.974687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>236.0</td>\n",
       "      <td>0.973218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>238.0</td>\n",
       "      <td>0.973197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>246.0</td>\n",
       "      <td>0.973175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>267.0</td>\n",
       "      <td>0.972922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>303.0</td>\n",
       "      <td>0.972722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>310.0</td>\n",
       "      <td>0.971270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>339.0</td>\n",
       "      <td>0.971201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>350.0</td>\n",
       "      <td>0.970604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>351.0</td>\n",
       "      <td>0.969490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>353.0</td>\n",
       "      <td>0.969480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>360.0</td>\n",
       "      <td>0.967086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>369.0</td>\n",
       "      <td>0.966131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>374.0</td>\n",
       "      <td>0.954787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>386.0</td>\n",
       "      <td>0.954734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>412.0</td>\n",
       "      <td>0.954680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>428.0</td>\n",
       "      <td>0.942874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>430.0</td>\n",
       "      <td>0.942754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>431.0</td>\n",
       "      <td>0.938245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>443.0</td>\n",
       "      <td>0.937939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>451.0</td>\n",
       "      <td>0.937821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>456.0</td>\n",
       "      <td>0.937811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>457.0</td>\n",
       "      <td>0.937554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>459.0</td>\n",
       "      <td>0.937543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>475.0</td>\n",
       "      <td>0.928423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>498.0</td>\n",
       "      <td>0.925581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>502.0</td>\n",
       "      <td>0.925048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>513.0</td>\n",
       "      <td>0.925027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>518.0</td>\n",
       "      <td>0.924293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>526.0</td>\n",
       "      <td>0.924047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>533.0</td>\n",
       "      <td>0.921976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>552.0</td>\n",
       "      <td>0.921976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>553.0</td>\n",
       "      <td>0.920364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>555.0</td>\n",
       "      <td>0.910046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>556.0</td>\n",
       "      <td>0.907539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>562.0</td>\n",
       "      <td>0.907539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>582.0</td>\n",
       "      <td>0.907517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>585.0</td>\n",
       "      <td>0.906487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>597.0</td>\n",
       "      <td>0.886119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>612.0</td>\n",
       "      <td>0.886039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>614.0</td>\n",
       "      <td>0.886028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>620.0</td>\n",
       "      <td>0.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>622.0</td>\n",
       "      <td>0.878699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>644.0</td>\n",
       "      <td>0.874269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>673.0</td>\n",
       "      <td>0.874155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>674.0</td>\n",
       "      <td>0.871136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>676.0</td>\n",
       "      <td>0.865382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>694.0</td>\n",
       "      <td>0.865211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>705.0</td>\n",
       "      <td>0.863922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>721.0</td>\n",
       "      <td>0.863790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>725.0</td>\n",
       "      <td>0.862123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>734.0</td>\n",
       "      <td>0.861899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>737.0</td>\n",
       "      <td>0.860608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>740.0</td>\n",
       "      <td>0.860538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>745.0</td>\n",
       "      <td>0.860538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>768.0</td>\n",
       "      <td>0.853320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>774.0</td>\n",
       "      <td>0.848962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>775.0</td>\n",
       "      <td>0.848367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>784.0</td>\n",
       "      <td>0.842516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>796.0</td>\n",
       "      <td>0.841593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>799.0</td>\n",
       "      <td>0.841547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>801.0</td>\n",
       "      <td>0.840726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>807.0</td>\n",
       "      <td>0.833161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>815.0</td>\n",
       "      <td>0.833161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>817.0</td>\n",
       "      <td>0.828687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>828.0</td>\n",
       "      <td>0.816614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>831.0</td>\n",
       "      <td>0.811816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>838.0</td>\n",
       "      <td>0.811641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>840.0</td>\n",
       "      <td>0.809006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>846.0</td>\n",
       "      <td>0.808382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>850.0</td>\n",
       "      <td>0.808131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>859.0</td>\n",
       "      <td>0.806633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>870.0</td>\n",
       "      <td>0.803064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>871.0</td>\n",
       "      <td>0.796050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>876.0</td>\n",
       "      <td>0.795493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>884.0</td>\n",
       "      <td>0.792043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>909.0</td>\n",
       "      <td>0.789761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>918.0</td>\n",
       "      <td>0.789253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>919.0</td>\n",
       "      <td>0.771532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>920.0</td>\n",
       "      <td>0.771299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>928.0</td>\n",
       "      <td>0.770180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>947.0</td>\n",
       "      <td>0.768953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>949.0</td>\n",
       "      <td>0.768928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>950.0</td>\n",
       "      <td>0.761512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>978.0</td>\n",
       "      <td>0.761285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>980.0</td>\n",
       "      <td>0.758295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>999.0</td>\n",
       "      <td>0.757634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1010.0</td>\n",
       "      <td>0.750641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1015.0</td>\n",
       "      <td>0.749208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1029.0</td>\n",
       "      <td>0.748874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1036.0</td>\n",
       "      <td>0.746311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1051.0</td>\n",
       "      <td>0.745534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1059.0</td>\n",
       "      <td>0.745415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.730003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1078.0</td>\n",
       "      <td>0.729763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1085.0</td>\n",
       "      <td>0.729739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1090.0</td>\n",
       "      <td>0.724873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1106.0</td>\n",
       "      <td>0.724873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1109.0</td>\n",
       "      <td>0.724873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.708446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1130.0</td>\n",
       "      <td>0.697557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1133.0</td>\n",
       "      <td>0.689876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1134.0</td>\n",
       "      <td>0.689491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1170.0</td>\n",
       "      <td>0.682890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1194.0</td>\n",
       "      <td>0.673543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1196.0</td>\n",
       "      <td>0.673491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1232.0</td>\n",
       "      <td>0.673075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1240.0</td>\n",
       "      <td>0.671238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1256.0</td>\n",
       "      <td>0.670986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1270.0</td>\n",
       "      <td>0.664290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.659840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1286.0</td>\n",
       "      <td>0.652593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1296.0</td>\n",
       "      <td>0.651046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1302.0</td>\n",
       "      <td>0.649604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>0.648396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1325.0</td>\n",
       "      <td>0.647378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1329.0</td>\n",
       "      <td>0.645033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1336.0</td>\n",
       "      <td>0.642765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1353.0</td>\n",
       "      <td>0.641452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1362.0</td>\n",
       "      <td>0.620038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1373.0</td>\n",
       "      <td>0.619951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1375.0</td>\n",
       "      <td>0.617409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.616315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.608245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1414.0</td>\n",
       "      <td>0.588999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1420.0</td>\n",
       "      <td>0.581590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>0.578452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1433.0</td>\n",
       "      <td>0.577290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1468.0</td>\n",
       "      <td>0.577068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1494.0</td>\n",
       "      <td>0.571816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1515.0</td>\n",
       "      <td>0.568320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1520.0</td>\n",
       "      <td>0.568202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.565560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1574.0</td>\n",
       "      <td>0.561214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1578.0</td>\n",
       "      <td>0.559635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1592.0</td>\n",
       "      <td>0.553382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1610.0</td>\n",
       "      <td>0.548380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1677.0</td>\n",
       "      <td>0.515879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1687.0</td>\n",
       "      <td>0.512113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>0.503510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1754.0</td>\n",
       "      <td>0.497004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1766.0</td>\n",
       "      <td>0.494482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1771.0</td>\n",
       "      <td>0.467124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1830.0</td>\n",
       "      <td>0.466344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1840.0</td>\n",
       "      <td>0.466344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1872.0</td>\n",
       "      <td>0.466109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1897.0</td>\n",
       "      <td>0.466109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1903.0</td>\n",
       "      <td>0.457974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1909.0</td>\n",
       "      <td>0.456442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1937.0</td>\n",
       "      <td>0.437125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2087.0</td>\n",
       "      <td>0.436511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2123.0</td>\n",
       "      <td>0.436133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2129.0</td>\n",
       "      <td>0.433497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2139.0</td>\n",
       "      <td>0.430570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2181.0</td>\n",
       "      <td>0.419049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2251.0</td>\n",
       "      <td>0.418272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2268.0</td>\n",
       "      <td>0.415546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2294.0</td>\n",
       "      <td>0.408367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2366.0</td>\n",
       "      <td>0.403028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2387.0</td>\n",
       "      <td>0.401724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2415.0</td>\n",
       "      <td>0.400086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2439.0</td>\n",
       "      <td>0.383642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2449.0</td>\n",
       "      <td>0.382306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2460.0</td>\n",
       "      <td>0.380769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2597.0</td>\n",
       "      <td>0.370222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2642.0</td>\n",
       "      <td>0.369494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2660.0</td>\n",
       "      <td>0.363317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2678.0</td>\n",
       "      <td>0.362016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2707.0</td>\n",
       "      <td>0.341122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2765.0</td>\n",
       "      <td>0.338397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>3024.0</td>\n",
       "      <td>0.334135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>3027.0</td>\n",
       "      <td>0.332079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>3036.0</td>\n",
       "      <td>0.320355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3765.0</td>\n",
       "      <td>0.293230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>3987.0</td>\n",
       "      <td>0.281253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>4563.0</td>\n",
       "      <td>0.277424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y\n",
       "0      21.0  0.998539\n",
       "1      23.0  0.998539\n",
       "2      25.0  0.998492\n",
       "3      59.0  0.998354\n",
       "4      63.0  0.998354\n",
       "5      77.0  0.997593\n",
       "6      81.0  0.997593\n",
       "7      85.0  0.997503\n",
       "8      90.0  0.997483\n",
       "9      91.0  0.997343\n",
       "10     96.0  0.997290\n",
       "11    100.0  0.997290\n",
       "12    112.0  0.997187\n",
       "13    127.0  0.996001\n",
       "14    139.0  0.994089\n",
       "15    143.0  0.993277\n",
       "16    144.0  0.990835\n",
       "17    145.0  0.984475\n",
       "18    162.0  0.983783\n",
       "19    173.0  0.981217\n",
       "20    184.0  0.976359\n",
       "21    206.0  0.976325\n",
       "22    230.0  0.976325\n",
       "23    231.0  0.974687\n",
       "24    236.0  0.973218\n",
       "25    238.0  0.973197\n",
       "26    246.0  0.973175\n",
       "27    267.0  0.972922\n",
       "28    303.0  0.972722\n",
       "29    310.0  0.971270\n",
       "30    339.0  0.971201\n",
       "31    350.0  0.970604\n",
       "32    351.0  0.969490\n",
       "33    353.0  0.969480\n",
       "34    360.0  0.967086\n",
       "35    369.0  0.966131\n",
       "36    374.0  0.954787\n",
       "37    386.0  0.954734\n",
       "38    412.0  0.954680\n",
       "39    428.0  0.942874\n",
       "40    430.0  0.942754\n",
       "41    431.0  0.938245\n",
       "42    443.0  0.937939\n",
       "43    451.0  0.937821\n",
       "44    456.0  0.937811\n",
       "45    457.0  0.937554\n",
       "46    459.0  0.937543\n",
       "47    475.0  0.928423\n",
       "48    498.0  0.925581\n",
       "49    502.0  0.925048\n",
       "50    513.0  0.925027\n",
       "51    518.0  0.924293\n",
       "52    526.0  0.924047\n",
       "53    533.0  0.921976\n",
       "54    552.0  0.921976\n",
       "55    553.0  0.920364\n",
       "56    555.0  0.910046\n",
       "57    556.0  0.907539\n",
       "58    562.0  0.907539\n",
       "59    582.0  0.907517\n",
       "60    585.0  0.906487\n",
       "61    597.0  0.886119\n",
       "62    612.0  0.886039\n",
       "63    614.0  0.886028\n",
       "64    620.0  0.885900\n",
       "65    622.0  0.878699\n",
       "66    644.0  0.874269\n",
       "67    673.0  0.874155\n",
       "68    674.0  0.871136\n",
       "69    676.0  0.865382\n",
       "70    694.0  0.865211\n",
       "71    705.0  0.863922\n",
       "72    721.0  0.863790\n",
       "73    725.0  0.862123\n",
       "74    734.0  0.861899\n",
       "75    737.0  0.860608\n",
       "76    740.0  0.860538\n",
       "77    745.0  0.860538\n",
       "78    768.0  0.853320\n",
       "79    774.0  0.848962\n",
       "80    775.0  0.848367\n",
       "81    784.0  0.842516\n",
       "82    796.0  0.841593\n",
       "83    799.0  0.841547\n",
       "84    801.0  0.840726\n",
       "85    807.0  0.833161\n",
       "86    815.0  0.833161\n",
       "87    817.0  0.828687\n",
       "88    828.0  0.816614\n",
       "89    831.0  0.811816\n",
       "90    838.0  0.811641\n",
       "91    840.0  0.809006\n",
       "92    846.0  0.808382\n",
       "93    850.0  0.808131\n",
       "94    859.0  0.806633\n",
       "95    870.0  0.803064\n",
       "96    871.0  0.796050\n",
       "97    876.0  0.795493\n",
       "98    884.0  0.792043\n",
       "99    909.0  0.789761\n",
       "100   918.0  0.789253\n",
       "101   919.0  0.771532\n",
       "102   920.0  0.771299\n",
       "103   928.0  0.770180\n",
       "104   947.0  0.768953\n",
       "105   949.0  0.768928\n",
       "106   950.0  0.761512\n",
       "107   978.0  0.761285\n",
       "108   980.0  0.758295\n",
       "109   999.0  0.757634\n",
       "110  1010.0  0.750641\n",
       "111  1015.0  0.749208\n",
       "112  1029.0  0.748874\n",
       "113  1036.0  0.746311\n",
       "114  1051.0  0.745534\n",
       "115  1059.0  0.745415\n",
       "116  1070.0  0.730003\n",
       "117  1078.0  0.729763\n",
       "118  1085.0  0.729739\n",
       "119  1090.0  0.724873\n",
       "120  1106.0  0.724873\n",
       "121  1109.0  0.724873\n",
       "122  1124.0  0.708446\n",
       "123  1130.0  0.697557\n",
       "124  1133.0  0.689876\n",
       "125  1134.0  0.689491\n",
       "126  1170.0  0.682890\n",
       "127  1194.0  0.673543\n",
       "128  1196.0  0.673491\n",
       "129  1232.0  0.673075\n",
       "130  1240.0  0.671238\n",
       "131  1256.0  0.670986\n",
       "132  1270.0  0.664290\n",
       "133  1280.0  0.659840\n",
       "134  1286.0  0.652593\n",
       "135  1296.0  0.651046\n",
       "136  1302.0  0.649604\n",
       "137  1314.0  0.648396\n",
       "138  1325.0  0.647378\n",
       "139  1329.0  0.645033\n",
       "140  1336.0  0.642765\n",
       "141  1353.0  0.641452\n",
       "142  1362.0  0.620038\n",
       "143  1373.0  0.619951\n",
       "144  1375.0  0.617409\n",
       "145  1386.0  0.616315\n",
       "146  1403.0  0.608245\n",
       "147  1414.0  0.588999\n",
       "148  1420.0  0.581590\n",
       "149  1429.0  0.578452\n",
       "150  1433.0  0.577290\n",
       "151  1468.0  0.577068\n",
       "152  1494.0  0.571816\n",
       "153  1515.0  0.568320\n",
       "154  1520.0  0.568202\n",
       "155  1557.0  0.565560\n",
       "156  1574.0  0.561214\n",
       "157  1578.0  0.559635\n",
       "158  1592.0  0.553382\n",
       "159  1610.0  0.548380\n",
       "160  1677.0  0.515879\n",
       "161  1687.0  0.512113\n",
       "162  1693.0  0.503510\n",
       "163  1754.0  0.497004\n",
       "164  1766.0  0.494482\n",
       "165  1771.0  0.467124\n",
       "166  1830.0  0.466344\n",
       "167  1840.0  0.466344\n",
       "168  1872.0  0.466109\n",
       "169  1897.0  0.466109\n",
       "170  1903.0  0.457974\n",
       "171  1909.0  0.456442\n",
       "172  1937.0  0.437125\n",
       "173  2087.0  0.436511\n",
       "174  2123.0  0.436133\n",
       "175  2129.0  0.433497\n",
       "176  2139.0  0.430570\n",
       "177  2181.0  0.419049\n",
       "178  2251.0  0.418272\n",
       "179  2268.0  0.415546\n",
       "180  2294.0  0.408367\n",
       "181  2366.0  0.403028\n",
       "182  2387.0  0.401724\n",
       "183  2415.0  0.400086\n",
       "184  2439.0  0.383642\n",
       "185  2449.0  0.382306\n",
       "186  2460.0  0.380769\n",
       "187  2597.0  0.370222\n",
       "188  2642.0  0.369494\n",
       "189  2660.0  0.363317\n",
       "190  2678.0  0.362016\n",
       "191  2707.0  0.341122\n",
       "192  2765.0  0.338397\n",
       "193  3024.0  0.334135\n",
       "194  3027.0  0.332079\n",
       "195  3036.0  0.320355\n",
       "196  3765.0  0.293230\n",
       "197  3987.0  0.281253\n",
       "198  4563.0  0.277424"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group 1\n",
    "raw_data = {\n",
    "            'ER': ['Positive'],\\\n",
    "            'PR': ['positive'],\\\n",
    "            'Her2': ['negative'],\\\n",
    "            'size_precise': [1.3],\\\n",
    "            'nodespos': [0],\\\n",
    "            'Age_@_Dx': [21],\\\n",
    "            'diff': ['grade 3']\n",
    "           }\n",
    "\n",
    "# # group 2\n",
    "# raw_data = {\n",
    "#             'ER': ['Positive'],\\\n",
    "#             'PR': ['positive'],\\\n",
    "#             'Her2': ['negative'],\\\n",
    "#             'Size': [\">\"],\\\n",
    "#             'Age_@_Dx': [21],\\\n",
    "#             'diff': ['m0']\n",
    "#            }\n",
    "\n",
    "# # group 3\n",
    "# raw_data = {\n",
    "#             'ER': ['Positive'],\\\n",
    "#             'PR': ['positive'],\\\n",
    "#             'Her2': ['negative'],\\\n",
    "#             'size_precise': [1.3],\\\n",
    "#             'nodespos': [0],\\\n",
    "#             'Age_@_Dx': [21],\\\n",
    "#             'T':['Tis'],\\\n",
    "#             'N': ['n0'],\\\n",
    "#             'M': ['m0']\n",
    "#            }\n",
    "\n",
    "\n",
    "z,df = survivalTable(\"group 1_layer 4_rsf\",raw_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "\n",
    "For prediction, a sample is dropped down each tree in the forest until it reaches a terminal node. Data in each terminal is used to non-parametrically estimate the survival and cumulative hazard function using the Kaplan-Meier and Nelson-Aalen estimator, respectively. In addition, a risk score can be computed that represents the expected number of events for one particular terminal node. The ensemble prediction is simply the average across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_values = X_test.values\n",
    "a = np.empty(X_test.shape[0], dtype=[(\"Age_@_Dx\", float), (\"nodespos\", float)])\n",
    "a[\"Age_@_Dx\"] = X_test_values[:, -1]\n",
    "a[\"nodespos\"] = X_test_values[:, -2]\n",
    "\n",
    "sort_idx = np.argsort(a, order=[\"nodespos\", \"Age_@_Dx\"])\n",
    "\n",
    "X_test_sel = pd.DataFrame(\n",
    "    X_test_values[(sort_idx[:1])],\n",
    "    columns=list(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict risk score\n",
    "pd.Series(rsf.predict(X_test_sel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Selection - (C index)\n",
    "\n",
    "'''\n",
    "In survival analysis, the hazard ratio (HR) is the ratio of the hazard rates corresponding\n",
    "to the conditions described by two levels of an explanatory variable. \n",
    "    For example, in a drug study, the treated population may die at twice the rate per unit time\n",
    "    as the control population. The hazard ratio would be 2, indicating higher hazard of death from the treatment. \n",
    "    Or in another study, men receiving the same treatment may suffer a certain complication ten times more\n",
    "    frequently per unit time than women, giving a hazard ratio of 10. - wiki\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = fit_and_score_features(X_test.values, Y_test)\n",
    "# pd.Series(scores, index=X_test.columns).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Performance of Survival Models\n",
    "Our test data is usually subject to censoring (only verified records are available, events happening in between is skipped), therefore metrics like root mean squared error or correlation are unsuitable. Instead, we use generalization of the area under the receiver operating characteristic (ROC) curve called Harrell's concordance index or c-index.\n",
    "\n",
    "The interpretation is identical to the traditional area under the ROC curve metric for binary classification:\n",
    "\n",
    "- a value of 0.5 denotes a random model,\n",
    "- a value of 1.0 denotes a perfect model,\n",
    "- a value of 0.0 denotes a perfectly wrong model.\n",
    "\n",
    "### Calculation matrix for CoxPHSurvivalAnalysis - Cox's proportional hazard's model\n",
    "\n",
    "tol is like the p value\n",
    "|1 - (new neg. log-likelihood / old neg. log-likelihood) | < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Determine features that are useful - Cox\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pipe = Pipeline([('encode', OneHotEncoder()),\n",
    "#                  ('select', SelectKBest(fit_and_score_features, k=3)),\n",
    "#                  ('model', CoxPHSurvivalAnalysis(alpha = 1e-6, tol= 1e-6))])\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'select__k': np.arange(1, X.shape[1] + 1)}\n",
    "# gcv = GridSearchCV(pipe, param_grid, return_train_score=True, cv=3, iid=True)\n",
    "# gcv.fit(X, Y)\n",
    "\n",
    "# pd.DataFrame(gcv.cv_results_).sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# pipe.set_params(**gcv.best_params_)\n",
    "# pipe.fit(X, Y)\n",
    "\n",
    "# encoder, transformer, final_estimator = [s[1] for s in pipe.steps]\n",
    "# pd.Series(final_estimator.coef_, index=encoder.encoded_columns_[transformer.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation-based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "perm = PermutationImportance(rsf, n_iter=15, random_state=random_state)\n",
    "perm.fit(X_test, Y_test)\n",
    "eli5.show_weights(perm, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_X_features = X_features.dropna(axis = 0, how ='any')  \n",
    "# print(\"Old data frame length:\", len(X_features)) \n",
    "# print(\"New data frame length:\", len(new_X_features)) \n",
    "# print(\"Number of rows with at least 1 NA value: \", (len(X_features)-len(new_X_features))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "    # YEAR = 5\n",
    "    # STYPE = \"OS\"\n",
    "    # # k = plotGraph(df_dict,YEAR, STYPE, 0)\n",
    "\n",
    "    # for survival in (\"DFS\", \"OS\", 'CSS'):\n",
    "    #     time_treatment, survival_prob_treatment = kaplan_meier_estimator(\n",
    "    #         df_dict['{}_years'.format(YEAR)][survival]['status'],\n",
    "    #         df_dict['{}_years'.format(YEAR)][survival]['{}_days'.format(survival)])\n",
    "\n",
    "    #     plt.step(time_treatment/365.25, survival_prob_treatment, where=\"post\",\n",
    "    #              label=\"Survival Type = {}\".format(survival))\n",
    "\n",
    "    # plt.ylabel(\"est. probability of survival $\\hat{S}(t)$\")\n",
    "    # plt.xlabel(\"time $t$ (Days)\")\n",
    "    # plt.title(\"{} Years Surivial Rate For Each Category\".format(YEAR))\n",
    "    # plt.grid(True)\n",
    "    # plt.legend(loc=\"best\")\n",
    "\n",
    "    # # plt.rcParams[\"figure.figsize\"] = (30,10)\n",
    "\n",
    "    # # for value in df_dict['{}_years'.format(YEAR)]['OS'][\"TNM_Stage\"].unique():\n",
    "    # #     mask = df_dict['{}_years'.format(YEAR)]['OS'][\"TNM_Stage\"] == value\n",
    "    # #     time_cell, survival_prob_cell = kaplan_meier_estimator(df_dict['{}_years'.format(YEAR)][STYPE]['status'][mask],\n",
    "    # #                                                            df_dict['{}_years'.format(YEAR)][STYPE]['{}_days'.format(STYPE)][mask])\n",
    "    # #     plt.step(time_cell, survival_prob_cell, where=\"post\",\n",
    "    # #              label= '{} (n = {})'.format(value, mask.sum()))\n",
    "\n",
    "    # # plt.ylabel(\"est. probability of survival $\\hat(t)$\")\n",
    "    # # plt.xlabel(\"time $t$\")\n",
    "    # # plt.grid(True)\n",
    "    # # plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = {\n",
    "#     'L2_reg': 10.0,\n",
    "#     'batch_norm': True,\n",
    "#     'dropout': 0.4,\n",
    "#     'hidden_layers_sizes': [25, 25],\n",
    "#     'learning_rate': 1e-05,\n",
    "#     'lr_decay': 0.001,\n",
    "#     'momentum': 0.9,\n",
    "#     'n_in': train_data['x'].shape[1],\n",
    "#     'standardize': True\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation: Not enough data per group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
