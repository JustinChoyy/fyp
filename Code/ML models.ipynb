{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import win32com.client\n",
    "import getpass\n",
    "import datetime\n",
    "import pywintypes\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "#ann model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "import math\n",
    "from sklearn.ensemble import IsolationForest\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ANN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    Returns all the data that needs to be used for ANN.\n",
    "    Output(3 dataframes): clinical data, clinical data (OHE), bills grouped by time period\n",
    "    \"\"\"\n",
    "    master = pd.read_pickle(\"C:\\\\SMU_v2\\\\price_timeperiod.pkl\")\n",
    "    \n",
    "    listToKeep = ['NRIC','dx_date','tstage','nstage', 'Mstage', 'ER', 'PR',\\\n",
    "               'Her2', 'size_precise', 'nodespos', 'Age_@_Dx']\n",
    "    \n",
    "    clinical = master[listToKeep]\n",
    "    \n",
    "    OHE = [i for i in clinical.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    x_clinical = pd.get_dummies(clinical,columns=OHE,dummy_na=True).reset_index(drop=True)\n",
    "    prices_grouped = master[[\"NRIC\",\"before_6m\", \"after_6m\", \"after_1y\", \"after_2y\", \"after_3y\", \"after_4y\",\n",
    "               \"after_5y\", \"after_6y\", \"after_7y\",\"after_8y\", \"after_9y\", \"after_10y\"]]\n",
    "    return clinical.reset_index(drop=True), x_clinical.reset_index(drop=True), prices_grouped.reset_index(drop=True)\n",
    "\n",
    "def scale_data(data,scale_obj):\n",
    "    \"\"\"\n",
    "    transforms then scales data according to min-max\n",
    "    \"\"\"\n",
    "#     data = data.apply(np.log1p)\n",
    "    prices_grouped_scaled = pd.DataFrame(scale_obj.fit_transform(data))\n",
    "    return prices_grouped_scaled\n",
    "\n",
    "def scale_data_reverse(data,scale_obj):\n",
    "    \"\"\"\n",
    "    returns a dataframe that reverses the min-max that was done previously\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(scale_obj.inverse_transform(data))\n",
    "#     predictions_scaled_reverse = data.apply(np.expm1)\n",
    "    return data\n",
    "\n",
    "def ann_structure(input_shape,output_units):\n",
    "    \"\"\"\n",
    "    function to declare ANN structure. just for code cleaniness\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(32, input_shape=(input_shape,)))         # input layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    \n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                        beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                                        moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                        beta_constraint=None, gamma_constraint=None)\n",
    ") \n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(output_units, activation=tf.nn.leaky_relu))   # one output layer with 1 outputs\n",
    "    return model\n",
    "\n",
    "def remove_out_of_range(data):\n",
    "    \"\"\"\n",
    "    determines index of data where there is no additional information\n",
    "    \"\"\"\n",
    "    y1 = data[data[\"after_1y\"].isnull()].index\n",
    "    y2 = data[data[\"after_2y\"].isnull()].index\n",
    "    y5 = data[data[\"after_5y\"].isnull()].index\n",
    "    y10 = data[data[\"after_10y\"].isnull()].index\n",
    "    return {\n",
    "        \"y1\":[4,y1], \n",
    "        \"y2\":[5,y2], \n",
    "        \"y5\":[8,y5], \n",
    "        \"y10\":[13,y10]}\n",
    "\n",
    "def remove_meaningless_data(data):\n",
    "    \"\"\"\n",
    "    returns index of all rows that do not add any additional input. aka all fields are 0\n",
    "    \"\"\"\n",
    "    return data[data.sum(axis=1)==0].index\n",
    "\n",
    "def drop_by_index(X,y,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    y = y.drop(indexes)\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    return(X,y)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 30:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "    \n",
    "def process_time_period(data,scope):\n",
    "    \"\"\"\n",
    "    Takes in yearly healthcare costs of patients and processes it into 1,2,5,10 year values\n",
    "    \"\"\"\n",
    "    y = pd.DataFrame()\n",
    "    y[\"6 months before\"] = data.iloc[:,0]\n",
    "    y[\"6 months after\"] = data.iloc[:,1]\n",
    "    y[\"1 year after\"] = data.iloc[:,2]\n",
    "    if scope != \"y1\":\n",
    "        y[\"2 years after\"] = data.iloc[:,3]\n",
    "        if scope != \"y2\":\n",
    "            y[\"5 years after\"] = data.iloc[:,4:7].sum(axis=1)\n",
    "            if scope != \"y5\":\n",
    "                y[\"10 years after\"] = data.iloc[:,7:].sum(axis=1)\n",
    "    return y\n",
    "    \n",
    "def make_prediction(all_users,user,model,mms):\n",
    "    \"\"\"\n",
    "    Given user data(dataframe) and the trained model, outputs the predicted values.\n",
    "    \n",
    "    Only works if all items in new user data has appeared at least once before\n",
    "    \"\"\"\n",
    "    all_users = all_users.reset_index().drop(columns=\"index\")\n",
    "    last_row = all_users.shape[0]\n",
    "    all_users = all_users.append(user)\n",
    "    all_users = all_users.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "    OHE = [i for i in all_users.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    usersOHE = pd.get_dummies(all_users,columns=OHE,dummy_na=True).reset_index().drop(columns=\"index\")\n",
    "    prediction_x = usersOHE\n",
    "#     print(prediction_x.shape)\n",
    "    pred = model.predict(prediction_x)\n",
    "    predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(pred),columns=[\"6 months before\",\"6 months after\",\n",
    "                                                                                   \"1 year after\",\"2 years after\",\n",
    "                                                                                   \"5 years after\",\"10 years after\"][:pred.shape[1]])\n",
    "    return pd.DataFrame([pd.DataFrame(predictions_scaled_reverse).iloc[last_row]]).reset_index().drop(columns=\"index\")\n",
    "\n",
    "def make_comparison(all_users,all_users_OHE,NRIC,bills,model,mms):\n",
    "    \"\"\"\n",
    "    Given a specific user, calculate out his actual cost and predicted costs\n",
    "    \"\"\"\n",
    "    x = all_users[all_users[\"NRIC\"] == NRIC]\n",
    "    pred = make_prediction(all_users,x,model,mms)\n",
    "    pred[\"Status\"] = \"Prediction\"\n",
    "    if pred.shape[1] == 7:\n",
    "        limit = 0\n",
    "    elif pred.shape[1] == 6:\n",
    "        limit = 1\n",
    "    else:\n",
    "        limit = 2\n",
    "    y_test = process_time_period(bills[all_users[\"NRIC\"] == NRIC],limit)\n",
    "    y_test[\"Status\"] = \"True data\"\n",
    "    y_test.columns = [\"6 months before\",\"6 months after\",\"1 year after\",\n",
    "                      \"2 years after\",\"5 years after\",\"10 years after\"][:pred.shape[1]-1] +[\"Status\"]\n",
    "    \n",
    "    to_return = y_test.append(pred)\n",
    "    return to_return\n",
    "\n",
    "def get_percentage(df1,df2,percentage):\n",
    "    \"\"\"\n",
    "    Given 2 dataframes, get the difference between the dataframes, \n",
    "    and return number of records that fall within a given percentage.\n",
    "    Eg: df1 contains 5 values [1,2,3,4,5]. Df2 contains [1,2,9,4,5]. 4 out of 5 values in \n",
    "    df1 fall within +- 5(percentage)% of the values in the same postion in df2. \n",
    "    Thus function will return 4/5 or 0.8\n",
    "    \"\"\"\n",
    "    process = lambda s1,s2: abs(s1-s2)/s2 < percentage \n",
    "    combined = df1.combine(df2, process)\n",
    "    total_count = (df1.shape[0] * df1.shape[1])\n",
    "    minus = sum([pd.value_counts(df2[i].values)[0] for i in df2.columns])\n",
    "    total_count -= minus\n",
    "    return combined.sum().sum() / total_count\n",
    "\n",
    "def display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse):\n",
    "    graph = pd.DataFrame(np.arange(0,2,.01),columns=[\"Percentage\"])\n",
    "    graph[\"viz\"] = graph.applymap(lambda x: get_percentage(predictions_scaled_reverse,y_test_scaled_reverse,x))\n",
    "    show = graph.plot.area(x=\"Percentage\")\n",
    "    show.set_title(\"Model performance ({})\".format(scope))\n",
    "    show.set_xlabel(\"Percentage Difference from Ground Truth\")\n",
    "    show.set_ylabel(\"Percentage of all our predictions\")\n",
    "    show.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    show.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    \n",
    "def get_data(scope,clinicalOHE, bills_grouped, outlier=False):\n",
    "    index = remove_indexes[scope]\n",
    "\n",
    "    X = clinicalOHE\n",
    "    y = bills_grouped.iloc[:,:index[0]]  \n",
    "\n",
    "    X = X.iloc[:,2:]\n",
    "    y = process_time_period(y.iloc[:,1:],scope)\n",
    "\n",
    "    print(\"Data shape original: {}\".format(X.shape[0]))\n",
    "    X,y = drop_by_index(X,y,index[1])\n",
    "\n",
    "    print(\"Data shape removing data out of scope: {}\".format(X.shape[0]))\n",
    "\n",
    "\n",
    "    meaningless = remove_meaningless_data(y)\n",
    "    X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "    print(\"Data shape meaningless data: {}\".format(X.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if outlier:\n",
    "        clf = IsolationForest(contamination=\"auto\",behaviour=\"new\",random_state=42)\n",
    "        out = clf.fit_predict(y)\n",
    "        out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "        remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "        X,y = drop_by_index(X,y,remove)\n",
    "\n",
    "        print(\"Data shape after removing outliers: {}\".format(X.shape[0]))\n",
    "\n",
    "    mms = MinMaxScaler()\n",
    "    y_scaled = scale_data(y,mms)\n",
    "    return X,y_scaled,mms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical, clinicalOHE, bills_grouped = read_data()\n",
    "\n",
    "remove_indexes = remove_out_of_range(bills_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#10 years\n",
    "scope = \"y10\"\n",
    "X,y,mms = get_data(scope,clinicalOHE,bills_grouped)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 100\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = pd.DataFrame(model.predict(X_test))\n",
    "predictions_scaled_reverse = scale_data_reverse(pred,mms)\n",
    "y_test_scaled_reverse = scale_data_reverse(y_test,mms)\n",
    "display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5 years\n",
    "outlier = False\n",
    "scope = \"y5\"\n",
    "X,y,mms = get_data(scope,clinicalOHE,bills_grouped)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 100\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = pd.DataFrame(model.predict(X_test))\n",
    "predictions_scaled_reverse = scale_data_reverse(pred,mms)\n",
    "y_test_scaled_reverse = scale_data_reverse(y_test,mms)\n",
    "display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2 years\n",
    "outlier = False\n",
    "scope = \"y2\"\n",
    "X,y,mms = get_data(scope,clinicalOHE,bills_grouped)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = pd.DataFrame(model.predict(X_test))\n",
    "predictions_scaled_reverse = scale_data_reverse(pred,mms)\n",
    "y_test_scaled_reverse = scale_data_reverse(y_test,mms)\n",
    "display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 year\n",
    "outlier = False\n",
    "scope = \"y1\"\n",
    "X,y,mms = get_data(scope,clinicalOHE,bills_grouped)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = pd.DataFrame(model.predict(X_test))\n",
    "predictions_scaled_reverse = scale_data_reverse(pred,mms)\n",
    "y_test_scaled_reverse = scale_data_reverse(y_test,mms)\n",
    "display_graph(scope,predictions_scaled_reverse,y_test_scaled_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
