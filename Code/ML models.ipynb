{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import win32com.client\n",
    "import getpass\n",
    "import datetime\n",
    "import pywintypes\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "#ann model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import math\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ANN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    Returns all the data that needs to be used for ANN.\n",
    "    Output(4 dataframes): all bills, clinical data, clinical data (OHE), bills grouped by time period\n",
    "    \"\"\"\n",
    "    bills_clean = pd.read_pickle('C:\\\\SMU_v2\\\\bills_output.pkl')\n",
    "    CDM = pd.read_pickle(\"C:\\\\SMU_v2\\\\clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "\n",
    "    clinical = CDM.drop(['dob','cause_of_death','death_age',\\\n",
    "                         'Date_for_DFS','Date_for_OS', 'Date_for_CSS',\\\n",
    "                         'Count_as_DFS', 'Count_as_OS','Count_as_CSS'], axis=1)\n",
    "\n",
    "    OHE = [i for i in clinical.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    x_clinical = pd.get_dummies(clinical,columns=OHE,dummy_na=True).reset_index().drop(columns=\"index\")\n",
    "    prices_grouped = pd.read_pickle(\"C:\\\\SMU_v2\\\\price_timeperiod.pkl\").reset_index().drop(columns=\"index\")\n",
    "    return bills_clean, CDM, x_clinical, prices_grouped\n",
    "\n",
    "def scale_data(data,scale_obj):\n",
    "    \"\"\"\n",
    "    scales data according to min-max\n",
    "    \"\"\"\n",
    "    prices_grouped_scaled = pd.DataFrame(scale_obj.fit_transform(data))\n",
    "    return prices_grouped_scaled\n",
    "\n",
    "def scale_data_reverse(data,scale_obj):\n",
    "    \"\"\"\n",
    "    returns a dataframe that reverses the min-max that was done previously\n",
    "    \"\"\"\n",
    "    predictions_scaled_reverse = pd.DataFrame(scale_obj.inverse_transform(data))\n",
    "    return predictions_scaled_reverse\n",
    "\n",
    "def ann_structure(input_shape,output_units):\n",
    "    \"\"\"\n",
    "    function to declare ANN structure. just for code cleaniness\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64, input_shape=(input_shape,)))         # input layer\n",
    "    model.add(layers.Dense(64, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(8, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(8, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(output_units, activation=tf.nn.leaky_relu))   # one output layer with 1 outputs\n",
    "    return model\n",
    "\n",
    "def remove_out_of_range(data):\n",
    "    \"\"\"\n",
    "    determines index of data where there is no additional information\n",
    "    \"\"\"\n",
    "    y1 = data[data[\"after_1y\"].isnull()].index\n",
    "    y2 = data[data[\"after_2y\"].isnull()].index\n",
    "    y5 = data[data[\"after_5y\"].isnull()].index\n",
    "    y10 = data[data[\"after_10y\"].isnull()].index\n",
    "    return {\n",
    "        \"y1\":[3,y1], \n",
    "        \"y2\":[4,y2], \n",
    "        \"y5\":[7,y5], \n",
    "        \"y10\":[12,y10]}\n",
    "\n",
    "def remove_meaningless_data(data):\n",
    "    \"\"\"\n",
    "    returns index of all rows that do not add any additional input. aka all fields are 0\n",
    "    \"\"\"\n",
    "    return data[data.sum(axis=1)==0].index\n",
    "\n",
    "def drop_by_index(X,y,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    y = y.drop(indexes)\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    y = y.reset_index().drop(columns=\"index\")\n",
    "    return(X,y)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 30:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills, clinical, clinicalOHE, bills_grouped = read_data()\n",
    "remove_indexes = remove_out_of_range(bills_grouped)\n",
    "to_drop = clinicalOHE[clinicalOHE[\"dx_date\"] == \"NA\"].index\n",
    "bills_grouped,clinicalOHE = drop_by_index(bills_grouped,clinicalOHE,to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 years\n",
    "outlier = True\n",
    "scope = \"y10\"\n",
    "index = remove_indexes[scope]\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y[\"6 months before\"] = y_small.iloc[:,0]\n",
    "y[\"6 months after\"] = y_small.iloc[:,1]\n",
    "y[\"1 year after\"] = y_small.iloc[:,2]\n",
    "y[\"2 years after\"] = y_small.iloc[:,3]\n",
    "y[\"5 years after\"] = y_small.iloc[:,4:7].sum(axis=1)\n",
    "y[\"10 years after\"] = y_small.iloc[:,7:].sum(axis=1)\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "    \n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "# model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "#           loss='mean_squared_error')\n",
    "# # Run the stochastic gradient descent for specified epochs\n",
    "# epochs = 50\n",
    "# filepath=\"weights.best.{}.h5\".format(scope)\n",
    "# callbacks_list = []\n",
    "# callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 years\n",
    "outlier = False\n",
    "scope = \"y10\"\n",
    "index = remove_indexes[scope]\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y[\"6 months before\"] = y_small.iloc[:,0]\n",
    "y[\"6 months after\"] = y_small.iloc[:,1]\n",
    "y[\"1 year after\"] = y_small.iloc[:,2]\n",
    "y[\"2 years after\"] = y_small.iloc[:,3]\n",
    "y[\"5 years after\"] = y_small.iloc[:,4:7].sum(axis=1)\n",
    "y[\"10 years after\"] = y_small.iloc[:,7:].sum(axis=1)\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5 years\n",
    "scope = \"y5\"\n",
    "index = remove_indexes[scope]\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y[\"6 months before\"] = y_small.iloc[:,0]\n",
    "y[\"6 months after\"] = y_small.iloc[:,1]\n",
    "y[\"1 year after\"] = y_small.iloc[:,2]\n",
    "y[\"2 years after\"] = y_small.iloc[:,3]\n",
    "y[\"5 years after\"] = y_small.iloc[:,4:7].sum(axis=1)\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2 years\n",
    "scope = \"y2\"\n",
    "index = remove_indexes[scope]\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y[\"6 months before\"] = y_small.iloc[:,0]\n",
    "y[\"6 months after\"] = y_small.iloc[:,1]\n",
    "y[\"1 year after\"] = y_small.iloc[:,2]\n",
    "y[\"2 years after\"] = y_small.iloc[:,3]\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 50\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "predictions = pd.DataFrame(pred)\n",
    "predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(predictions))\n",
    "y_test_scaled_reverse = pd.DataFrame(mms.inverse_transform(y_test))\n",
    "print(predictions_scaled_reverse.head())\n",
    "print(y_test_scaled_reverse.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
