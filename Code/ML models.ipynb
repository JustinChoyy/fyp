{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import win32com.client\n",
    "import getpass\n",
    "import datetime\n",
    "import pywintypes\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "#ann model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import math\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ANN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    Returns all the data that needs to be used for ANN.\n",
    "    Output(4 dataframes): all bills, clinical data, clinical data (OHE), bills grouped by time period\n",
    "    \"\"\"\n",
    "    bills_clean = pd.read_pickle('C:\\\\SMU_v2\\\\bills_output.pkl')\n",
    "    CDM = pd.read_pickle(\"C:\\\\SMU_v2\\\\clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "\n",
    "    clinical = CDM.drop(['dob','cause_of_death','death_age',\\\n",
    "                         'Date_for_DFS','Date_for_OS', 'Date_for_CSS',\\\n",
    "                         'Count_as_DFS', 'Count_as_OS','Count_as_CSS'], axis=1)\n",
    "\n",
    "    OHE = [i for i in clinical.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    x_clinical = pd.get_dummies(clinical,columns=OHE,dummy_na=True).reset_index().drop(columns=\"index\")\n",
    "    prices_grouped = pd.read_pickle(\"C:\\\\SMU_v2\\\\price_timeperiod.pkl\").reset_index().drop(columns=\"index\")\n",
    "    return bills_clean, clinical, x_clinical, prices_grouped\n",
    "\n",
    "def scale_data(data,scale_obj):\n",
    "    \"\"\"\n",
    "    scales data according to min-max\n",
    "    \"\"\"\n",
    "    prices_grouped_scaled = pd.DataFrame(scale_obj.fit_transform(data))\n",
    "    return prices_grouped_scaled\n",
    "\n",
    "def scale_data_reverse(data,scale_obj):\n",
    "    \"\"\"\n",
    "    returns a dataframe that reverses the min-max that was done previously\n",
    "    \"\"\"\n",
    "    predictions_scaled_reverse = pd.DataFrame(scale_obj.inverse_transform(data))\n",
    "    return predictions_scaled_reverse\n",
    "\n",
    "def ann_structure(input_shape,output_units):\n",
    "    \"\"\"\n",
    "    function to declare ANN structure. just for code cleaniness\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(32, input_shape=(input_shape,)))         # input layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    \n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    ") \n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(32, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(16, activation=tf.nn.leaky_relu))      # one hidden layer\n",
    "    model.add(layers.Dense(output_units, activation=tf.nn.leaky_relu))   # one output layer with 1 outputs\n",
    "    return model\n",
    "\n",
    "def remove_out_of_range(data):\n",
    "    \"\"\"\n",
    "    determines index of data where there is no additional information\n",
    "    \"\"\"\n",
    "    y1 = data[data[\"after_1y\"].isnull()].index\n",
    "    y2 = data[data[\"after_2y\"].isnull()].index\n",
    "    y5 = data[data[\"after_5y\"].isnull()].index\n",
    "    y10 = data[data[\"after_10y\"].isnull()].index\n",
    "    return {\n",
    "        \"y1\":[3,y1], \n",
    "        \"y2\":[4,y2], \n",
    "        \"y5\":[7,y5], \n",
    "        \"y10\":[12,y10]}\n",
    "\n",
    "def remove_meaningless_data(data):\n",
    "    \"\"\"\n",
    "    returns index of all rows that do not add any additional input. aka all fields are 0\n",
    "    \"\"\"\n",
    "    return data[data.sum(axis=1)==0].index\n",
    "\n",
    "def drop_by_index(X,y,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    y = y.drop(indexes)\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    y = y.reset_index().drop(columns=\"index\")\n",
    "    return(X,y)\n",
    "\n",
    "def scheduler(epoch):\n",
    "    \"\"\"\n",
    "    to reduce learning rate as epoch number increases\n",
    "    \"\"\"\n",
    "    if epoch < 30:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * math.exp(0.1 * (10 - int(epoch)))\n",
    "    \n",
    "def process_time_period(data,limit=0):\n",
    "    \"\"\"\n",
    "    Takes in yearly healthcare costs of patients and processes it into 1,2,5,10 year values\n",
    "    \"\"\"\n",
    "    y = pd.DataFrame()\n",
    "    y[\"6 months before\"] = data.iloc[:,0]\n",
    "    y[\"6 months after\"] = data.iloc[:,1]\n",
    "    y[\"1 year after\"] = data.iloc[:,2]\n",
    "    y[\"2 years after\"] = data.iloc[:,3]\n",
    "    if limit < 1:\n",
    "        y[\"5 years after\"] = data.iloc[:,4:7].sum(axis=1)\n",
    "        if limit < 2:\n",
    "            y[\"10 years after\"] = data.iloc[:,7:].sum(axis=1)\n",
    "    return y\n",
    "    \n",
    "def make_prediction(all_users,user,model,mms):\n",
    "    \"\"\"\n",
    "    Given user data(dataframe) and the trained model, outputs the predicted values.\n",
    "    \n",
    "    Only works if all items in new user data has appeared at least once before\n",
    "    \"\"\"\n",
    "    all_users = all_users.reset_index().drop(columns=\"index\")\n",
    "    last_row = all_users.shape[0]\n",
    "    all_users = all_users.append(user)\n",
    "    all_users = all_users.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "    OHE = [i for i in all_users.columns if not (i in  [\"NRIC\", 'Age_@_Dx', 'size_precise', 'nodespos','dx_date'])]\n",
    "    usersOHE = pd.get_dummies(all_users,columns=OHE,dummy_na=True).reset_index().drop(columns=\"index\")\n",
    "    prediction_x = usersOHE\n",
    "    print(prediction_x.shape)\n",
    "    pred = model.predict(prediction_x)\n",
    "    predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(pred),columns=[\"6 months before\",\"6 months after\",\"1 year after\",\"2 years after\",\"5 years after\",\"10 years after\"][:pred.shape[1]])\n",
    "    return pd.DataFrame([pd.DataFrame(predictions_scaled_reverse).iloc[last_row]]).reset_index().drop(columns=\"index\")\n",
    "\n",
    "def make_comparison(all_users,all_users_OHE,NRIC,bills,model,mms):\n",
    "    \"\"\"\n",
    "    Given a specific user, calculate out his actual cost and predicted costs\n",
    "    \"\"\"\n",
    "    x = all_users[all_users[\"NRIC\"] == NRIC]\n",
    "    pred = make_prediction(all_users,x,model,mms)\n",
    "    pred[\"Status\"] = \"Prediction\"\n",
    "    if pred.shape[1] == 7:\n",
    "        limit = 0\n",
    "    elif pred.shape[1] == 6:\n",
    "        limit = 1\n",
    "    else:\n",
    "        limit = 2\n",
    "    y_test = process_time_period(bills[all_users[\"NRIC\"] == NRIC],limit)\n",
    "    y_test[\"Status\"] = \"True data\"\n",
    "    y_test.columns = [\"6 months before\",\"6 months after\",\"1 year after\",\"2 years after\",\"5 years after\",\"10 years after\"][:pred.shape[1]-1] +[\"Status\"]\n",
    "    \n",
    "    to_return = y_test.append(pred)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills, clinical, clinicalOHE, bills_grouped = read_data()\n",
    "\n",
    "to_drop = clinicalOHE[clinicalOHE[\"dx_date\"] == \"NA\"].index\n",
    "bills_grouped,clinicalOHE = drop_by_index(bills_grouped,clinicalOHE,to_drop)\n",
    "remove_indexes = remove_out_of_range(bills_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#10 years\n",
    "outlier = True\n",
    "scope = \"y10\"\n",
    "index = remove_indexes[scope]\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "print(\"Data shape original: {}\".format(X.shape[0]))\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "print(\"Data shape removing data out of scope: {}\".format(X.shape[0]))\n",
    "y = process_time_period(y_small)\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "print(\"Data shape remove data that will cause errors: {}\".format(X.shape[0]))\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "print(\"Data shape meaningless data: {}\".format(X.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "if outlier:\n",
    "    clf = IsolationForest(contamination=\"auto\",behaviour=\"new\",random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "    \n",
    "    print(\"Data shape after removing outliers: {}\".format(X.shape[0]))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 100\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "# callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "predictions = pd.DataFrame(pred)\n",
    "predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(predictions),columns=[\"6 months before\",\"6 months after\",\"1 year after\",\"2 years after\",\"5 years after\",\"10 years after\"])\n",
    "y_test_scaled_reverse = pd.DataFrame(mms.inverse_transform(y_test),columns=[\"6 months before\",\"6 months after\",\"1 year after\",\"2 years after\",\"5 years after\",\"10 years after\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5 years\n",
    "scope = \"y5\"\n",
    "index = remove_indexes[scope]\n",
    "outlier = True\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "print(\"Data shape original: {}\".format(X.shape[0]))\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "print(\"Data shape removing data out of scope: {}\".format(X.shape[0]))\n",
    "y = process_time_period(y_small,1)\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "print(\"Data shape remove data that will cause errors: {}\".format(X.shape[0]))\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "print(\"Data shape meaningless data: {}\".format(X.shape[0]))\n",
    "\n",
    "if outlier:\n",
    "    clf = IsolationForest(contamination=\"auto\",behaviour=\"new\",random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)    \n",
    "    print(\"Data shape after removing outliers: {}\".format(X.shape[0]))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 200\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "predictions = pd.DataFrame(pred)\n",
    "predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(predictions),columns=[\"6 months before\",\"6 months after\",\"1 year after\",\"2 years after\",\"5 years after\"])\n",
    "y_test_scaled_reverse = pd.DataFrame(mms.inverse_transform(y_test),columns=[\"6 months before\",\"6 months after\",\"1 year after\",\"2 years after\",\"5 years after\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled_reverse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#10 years\n",
    "outlier = False\n",
    "scope = \"y10\"\n",
    "index = remove_indexes[scope]\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "y = process_time_period(y_small,2)\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "if outlier:\n",
    "    clf = IsolationForest(random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y_scaled = drop_by_index(X,y_scaled,remove)\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 200\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled_reverse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2 years\n",
    "scope = \"y2\"\n",
    "index = remove_indexes[scope]\n",
    "index = remove_indexes[scope]\n",
    "outlier = True\n",
    "\n",
    "y = bills_grouped.iloc[:,:index[0]]  \n",
    "X = clinicalOHE.drop(columns=[\"NRIC\",\"dx_date\"])\n",
    "\n",
    "print(\"Data shape original: {}\".format(X.shape[0]))\n",
    "\n",
    "X,y_small = drop_by_index(X,y,index[1])\n",
    "\n",
    "print(\"Data shape removing data out of scope: {}\".format(X.shape[0]))\n",
    "y = pd.DataFrame()\n",
    "y[\"6 months before\"] = y_small.iloc[:,0]\n",
    "y[\"6 months after\"] = y_small.iloc[:,1]\n",
    "y[\"1 year after\"] = y_small.iloc[:,2]\n",
    "y[\"2 years after\"] = y_small.iloc[:,3]\n",
    "\n",
    "to_drop = X[X[\"size_precise\"].isnull() | X[\"nodespos\"].isnull()].index\n",
    "X,y = drop_by_index(X,y,to_drop)\n",
    "print(\"Data shape remove data that will cause errors: {}\".format(X.shape[0]))\n",
    "\n",
    "meaningless = remove_meaningless_data(y)\n",
    "X,y = drop_by_index(X,y,meaningless)\n",
    "\n",
    "print(\"Data shape meaningless data: {}\".format(X.shape[0]))\n",
    "\n",
    "\n",
    "if outlier:\n",
    "    clf = IsolationForest(contamination=\"auto\",behaviour=\"new\",random_state=42)\n",
    "    out = clf.fit_predict(y)\n",
    "    out_df = pd.DataFrame(out,columns=[\"outlier\"])\n",
    "    remove = out_df[out_df[\"outlier\"] ==-1].index\n",
    "    X,y = drop_by_index(X,y,remove)\n",
    "    \n",
    "    print(\"Data shape after removing outliers: {}\".format(X.shape[0]))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "y_scaled = scale_data(y,mms)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.33, random_state=42)\n",
    "\n",
    "model = ann_structure(X.shape[1],y_scaled.shape[1])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "          loss='mean_squared_error')\n",
    "# Run the stochastic gradient descent for specified epochs\n",
    "epochs = 100\n",
    "filepath=\"weights.best.{}.h5\".format(scope)\n",
    "callbacks_list = []\n",
    "callbacks_list.append(ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True))\n",
    "callbacks_list.append(LearningRateScheduler(scheduler))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, callbacks = callbacks_list, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "predictions = pd.DataFrame(pred)\n",
    "predictions_scaled_reverse = pd.DataFrame(mms.inverse_transform(predictions))\n",
    "y_test_scaled_reverse = pd.DataFrame(mms.inverse_transform(y_test))\n",
    "print(predictions_scaled_reverse.head())\n",
    "print(y_test_scaled_reverse.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assuming model is chosen and trained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
