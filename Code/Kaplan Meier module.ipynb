{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pickle\n",
    "import fnmatch\n",
    "import os\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_column',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM_to_df(KM_object):\n",
    "    \n",
    "    # Process the summary as string\n",
    "    \n",
    "    summary_lines_list = str(KM_object.summary).split(\"\\n\")\n",
    "    \n",
    "    header = [\"time\", \"events\", \"at_risk\",  \"estimate\",  \"std_error\",  \"95%_CI_lower\",  \"95%_CI_upper\"]\n",
    "    rows = summary_lines_list[6:]\n",
    "    \n",
    "    row_values = []\n",
    "    \n",
    "    for row in rows:\n",
    "        \n",
    "        elements = row.split(\" \")\n",
    "        tmp = []\n",
    "        for element in elements:\n",
    "            if element.isnumeric() or (\".\" in element):\n",
    "                tmp.append(element)\n",
    "                \n",
    "        row_values.append(tmp)\n",
    "        \n",
    "    #Build df\n",
    "    output_df = pd.DataFrame(row_values, columns=header)\n",
    "                \n",
    "    return output_df\n",
    "\n",
    "def drop_by_index(X,indexes):\n",
    "    \"\"\"\n",
    "    helper function to drop rows of dataframe and return new dataframe without those rows with indexes resetted\n",
    "    \"\"\"\n",
    "    X = X.drop(indexes)\n",
    "    X = X.reset_index().drop(columns=\"index\")\n",
    "    return(X)\n",
    "\n",
    "def dataSetting(dropCol,FILE_FOLDER = \"C:\\\\SMU_v2\\\\\"):\n",
    "    '''\n",
    "    function to read the pkl from from datasource\n",
    "        1. Remove dx_date that is NULL.\n",
    "        2. Drop all rows where crucial fields for X_features are NULL.\n",
    "        3. Convert Date columns into datetime format\n",
    "        4. Derive OS, CSS, DFS days based on dx_date\n",
    "        5. Create status column to indicate if the patient is dead or alive base on if death_age exists\n",
    "    '''\n",
    "    df = pd.read_pickle(FILE_FOLDER + \"clinical_output.pkl\").reset_index().drop(columns=\"index\")\n",
    "    to_drop = df[df['dx_date']==\"NA\"].index\n",
    "    df = drop_by_index(df,to_drop)\n",
    "\n",
    "    df.drop(columns=dropCol,inplace = True)\n",
    "\n",
    "    # drop all rows where dates are null\n",
    "    df.dropna(axis=0,\\\n",
    "                    subset=['Date_for_DFS','Date_for_OS','Date_for_CSS','dx_date','Age_@_Dx'],\\\n",
    "                    inplace=True)\n",
    "    \n",
    "    # convert all datetime in dataframe into dateime format for processing\n",
    "    df[\"Date_for_DFS\"] = pd.to_datetime(df[\"Date_for_DFS\"])\n",
    "    df[\"Date_for_OS\"] = pd.to_datetime(df[\"Date_for_OS\"])\n",
    "    df[\"Date_for_CSS\"] = pd.to_datetime(df[\"Date_for_CSS\"])\n",
    "    df[\"dx_date\"] = pd.to_datetime(df[\"dx_date\"])\n",
    "    df['last_seen']= pd.to_datetime(df[\"dx_date\"])\n",
    "    df['dob']= pd.to_datetime(df[\"dx_date\"])\n",
    "\n",
    "    # calculate in days\n",
    "    df[\"DFS_days\"] = (df[\"Date_for_DFS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"OS_days\"] = (df[\"Date_for_OS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "    df[\"CSS_days\"] = (df[\"Date_for_CSS\"] - df['dx_date'] )/np.timedelta64(1, 'D')\n",
    "\n",
    "    # alive or dead\n",
    "    df['status'] = np.where(df['Count_as_OS'] == \"dead\", False, True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def ComputeYears(df, Year_list):\n",
    "    '''\n",
    "    Create a list to contain df for different years of survival\n",
    "    The df will filter those patient that has deceased or days of survival longer than the defined years.\n",
    "    '''\n",
    "\n",
    "    df_dict = {}\n",
    "\n",
    "    for i in Year_list:\n",
    "        tmp = {}\n",
    "        for x in list([\"DFS\", \"CSS\", \"OS\"]):\n",
    "            df['{}_{}_years'.format(x, i)] = np.where(\n",
    "                                                      np.logical_or(df['death_age'] > 0,\\\n",
    "                                                      df['{}_days'.format(x)]/(365.25*i) >= i),\\\n",
    "                                                      True,False)\n",
    "            tmp[x] = df[df['{}_{}_years'.format(x, i)] == True]\n",
    "        df_dict['{}_years'.format(i)] = tmp\n",
    "    return df_dict\n",
    "\n",
    "def train_test(X, Y, test_size = 0.33, random_state = 42):\n",
    "    '''\n",
    "    Splitting the dataset into the Training set and Test set\n",
    "    '''\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y,  test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def settingXY(df, X_features, Y_features, OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\", name=\"\"):\n",
    "    '''\n",
    "    This function returns the X and Y features need for model training\n",
    "        - The function also generates one pkl that contains the One Hot Encoder for new raw data \n",
    "    \n",
    "    X_features = features to use for X\n",
    "    Y_features = features to use for Y \n",
    "    YEAR = years of patient record interested\n",
    "    SYTPE = survival type (OS, DFS, CSS)\n",
    "    OHE_LOCATION = location to store the pkl file\n",
    "    '''\n",
    "\n",
    "    X = df[X_features]\n",
    "    Y = df[Y_features]\n",
    "\n",
    "    # Save enconder so that we can OHE new data\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    \n",
    "    # OHE for probability\n",
    "    X = enc.transform(X)\n",
    "    with open(OHE_LOCATION + name + '_encoder.pickle', 'wb') as f:\n",
    "        pickle.dump(enc, f) \n",
    "                  \n",
    "    # convert Y to structured array\n",
    "    s = Y.dtypes\n",
    "    Y = np.array([tuple(x) for x in Y.values], dtype=list(zip(s.index, s)))\n",
    "   \n",
    "    return X, Y\n",
    "def layeredData(df, group_dict,y_features, YEAR, STYPE):\n",
    "    \n",
    "    '''\n",
    "        this function generates the dataframe required for specific groups we hope to analyze\n",
    "        there are total 3 different groups but group 3 consist of multiple subgroups which leads a total of 5\n",
    "        dataframe.\n",
    "        Group 1: patient with stage 4 cancer\n",
    "        Group 2: patient which unknown records or at initial diagnosis stage\n",
    "        Group 3: make up of patient that does not belong to the groups above\n",
    "    '''\n",
    "    model_data_dict = {}\n",
    "    TO_USE = df['{}_years'.format(YEAR)][STYPE]\n",
    "    \n",
    "    print(\"Overall initial size: {} \\n\".format(TO_USE.shape[0]))\n",
    "        \n",
    "    for key,value in group_dict.items():\n",
    "        TO_USE_COPY = TO_USE.copy()\n",
    "\n",
    "        tmp = {}\n",
    "        \n",
    "        waves = value['wave']\n",
    "    \n",
    "        if key != \"group 3\":\n",
    "            # for group 1 and group 2 select rows that contains either stage 4/non invasive in Stage\n",
    "            TO_USE_COPY = TO_USE_COPY.loc[TO_USE_COPY['Stage'] == group_dict[key]['stage'][0]]\n",
    "        else:\n",
    "            # for group 3 do not select rows that contains either stage 4 or non invasive in c_Staging or p_Staging\n",
    "            stage = np.logical_and(TO_USE_COPY['Stage'] != group_dict[key]['stage'][0],\\\n",
    "                                    TO_USE_COPY['Stage'] != group_dict[key]['stage'][1])\n",
    "            \n",
    "            TO_USE_COPY = TO_USE_COPY.loc[stage]\n",
    "            \n",
    "        print(\"{} data size: {}\".format(key,len(TO_USE_COPY)))\n",
    "        \n",
    "        for wave in waves:\n",
    "            TO_USE_COPY2 = TO_USE_COPY.copy()\n",
    "            TO_USE_COPY2 = TO_USE_COPY2[waves[wave] + y_features]\n",
    "            \n",
    "            len_before = len(TO_USE_COPY2)\n",
    "            print(\"\\t{} data size before dropping nan: {}\".format(wave,len_before))\n",
    "            \n",
    "            TO_USE_COPY2.dropna(axis=0,subset=waves[wave]+ y_features, inplace=True)\n",
    "            TO_USE_COPY2.reset_index(drop=True)\n",
    "\n",
    "            len_after = len(TO_USE_COPY2)\n",
    "            print(\"\\t\\t after dropping nan: {}\".format(len_after))\n",
    "            \n",
    "            for i in waves[wave]:\n",
    "                if not (i in ['nodespos','Age_@_Dx','size_precise']):\n",
    "                    TO_USE_COPY2.loc[:,i] = TO_USE_COPY2[i].astype(\"category\")\n",
    "                else:\n",
    "                    TO_USE_COPY2.loc[:,i] = TO_USE_COPY2[i].astype(\"float32\")\n",
    "            \n",
    "            X, Y = settingXY(TO_USE_COPY2, waves[wave], y_features,name= \"{}_{}\".format(key,wave))   \n",
    "            \n",
    "            TO_USE_COPY2.to_pickle(\"C:\\\\SMU_v2\\\\Layered Folder\\\\{}_{}.pkl\".format(key,wave))\n",
    "            \n",
    "            tmp[wave] = {\n",
    "                            \"X\": X,\\\n",
    "                            \"Y\":Y      \n",
    "                        }    \n",
    "    \n",
    "        model_data_dict[key] = tmp\n",
    "        \n",
    "        \n",
    "    return model_data_dict\n",
    "\n",
    "def loadOHE(df,OHE_LOCATION = \"C:\\\\SMU_v2\\\\OHE\\\\\", name=\"\"):\n",
    "    '''\n",
    "    load enconder to OHE new raw data for prediction\n",
    "    '''\n",
    "    with open(OHE_LOCATION + name + '_encoder.pickle', 'rb') as f:\n",
    "        enc = pickle.load(f) \n",
    "    \n",
    "    #type case object to category\n",
    "    typeCastList = list(df.select_dtypes(include=[object]).columns)\n",
    "    df[typeCastList] = df[typeCastList].astype(\"category\")\n",
    "    OHE_New_Data = enc.transform(df)\n",
    "    \n",
    "    return OHE_New_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\SMU_v2\\\\Layered Folder\\\\'\n",
    "\n",
    "y_features = list(['status','OS_days'])\n",
    "\n",
    "if len(fnmatch.filter(os.listdir(path), '*.pkl')) > 0:\n",
    "    \n",
    "    model_data_dict = {}\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.pkl' in file:\n",
    "                group = str(file).split(\"_\")[0]\n",
    "                wave = str(file).split(\"_\")[1].split(\".\")[0]\n",
    "                tmp = pd.read_pickle(path+file)\n",
    "                \n",
    "                x_features = [i for i in tmp.columns if i not in y_features]\n",
    "                \n",
    "                X, Y = settingXY(tmp, x_features, y_features,name= \"{}_{}\".format(group,wave))  \n",
    "                \n",
    "                Y = Y.tolist()\n",
    "                Y = pd.DataFrame(Y, columns=['Status','OS_days'])\n",
    "                if not (group in model_data_dict): \n",
    "                    model_data_dict[group] = {wave : { \"X\": X,\\\n",
    "                                                       \"Y\":Y\n",
    "                                                      }}\n",
    "                else:\n",
    "                    model_data_dict[group].update({wave : { \"X\": X,\\\n",
    "                                                            \"Y\":Y\n",
    "                                                          }} )\n",
    "                del X\n",
    "                del Y\n",
    "                del tmp\n",
    "    print(\"Data loaded!\")                \n",
    "else:\n",
    "    # Data Processing\n",
    "    listToDrop = ['NRIC','dob','Has Bills?','Side','Hospital','KKH','NCCS','SGH','END_OF_ENTRY']\n",
    "    clinical = dataSetting(listToDrop)\n",
    "    print(clinical.shape)\n",
    "\n",
    "    # Data of our interest are 5 and 10 years, patient that are new \n",
    "    # (does not have sufficient records will disturb and mess up our accuracy level\n",
    "    # only return data that has longer timeframe than the given interval\n",
    "\n",
    "    year_list = list([1,5,10])\n",
    "    df_dict = ComputeYears(clinical,year_list)\n",
    "\n",
    "    # Display shape of data after filtering\n",
    "    for i in df_dict: \n",
    "        for s_type in df_dict[i]:\n",
    "            print(\"Year: {}, survival category: {}, size: {}\".format(i,s_type,df_dict[i][s_type].shape[0]))\n",
    "            \n",
    "    YEAR = 1\n",
    "    STYPE = \"OS\"\n",
    "\n",
    "    group_dict = { \n",
    "                    \"group 1\": {\n",
    "                                 \"stage\": ['stage 4'],\\\n",
    "                                 'wave': {\n",
    "                                             \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                             \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                         'T (no subgroup)', 'N (no subgroup)'],\\\n",
    "                                             \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N'],\\\n",
    "                                             \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise', 'nodespos']\n",
    "                                         }\n",
    "                               },\\\n",
    "                    \"group 2\": {\n",
    "                                 'stage': ['dcis/lcis non-invasive'],\\\n",
    "                                 'wave': {\n",
    "                                             \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Size'],\\\n",
    "                                             \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','size_precise']\n",
    "                                         }\n",
    "                               },\\\n",
    "                    \"group 3\": {\n",
    "                                 \"stage\": ['stage 4','dcis/lcis non-invasive'],\\\n",
    "                                 'wave': {\n",
    "                                             \"layer 1\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2','Stage'],\\\n",
    "                                             \"layer 2\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2',\\\n",
    "                                                         'T (no subgroup)', 'N (no subgroup)', 'M (no subgroup)'],\\\n",
    "                                             \"layer 3\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'T', 'N', 'M'],\\\n",
    "                                             \"layer 4\": ['Age_@_Dx', 'diff', 'ER', 'PR','Her2', 'size_precise',\\\n",
    "                                                         'nodespos','M']\n",
    "                                         }\n",
    "                               },\n",
    "                    }\n",
    "    model_data_dict = layeredData(df_dict, group_dict,y_features,YEAR, STYPE)\n",
    "    print(\"Processing Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_dict['group 3']['layer 1']['Y'].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaplan Meier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", palette=\"colorblind\", color_codes=True)\n",
    "\n",
    "from survive import datasets\n",
    "from survive import SurvivalData\n",
    "from survive import KaplanMeier, Breslow, NelsonAalen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Survival Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_surv_obj(survival_type, years, df_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function builds the survival object to be processed by kaplan meier model to return kaplan meier df\n",
    "    \"\"\"\n",
    "    \n",
    "    survival_type = str(survival_type)\n",
    "    years = str(years)\n",
    "    \n",
    "    survival_df = df_dict[years + \"_years\"][survival_type]\n",
    "    \n",
    "    Time_df = survival_df.loc[:,[survival_type + \"_days\"]]\n",
    "    Time_df[survival_type + \"_years\"] = Time_df[survival_type + \"_days\"]/365.25\n",
    "    Time_df[\"status\"] = survival_df[\"status_\" + survival_type]\n",
    "    Time_df.head()\n",
    "\n",
    "    return SurvivalData(time= (survival_type+ \"_years\"), status=\"status\", data=Time_df)\n",
    "\n",
    "#Build Input DF\n",
    "surv = build_surv_obj(survival_type=\"OS\", years=10, df_dict=df_dict)\n",
    "OS_km = KaplanMeier()\n",
    "OS_km.fit(surv)\n",
    "\n",
    "#Plot Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "OS_km.plot()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = KM_to_df(OS_km)\n",
    "o.to_csv(\"C:\\\\Users\\\\LINGXING\\\\Desktop\\\\GIT\\\\fyp\\\\Code\\\\km.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = pd.read_csv(\"C:\\\\Users\\\\LINGXING\\\\Desktop\\\\GIT\\\\fyp\\\\Code\\\\km.csv\")\n",
    "i.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disease Free Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Input DF\n",
    "surv = build_surv_obj(survival_type=\"DFS\",years=10, df_dict=df_dict)\n",
    "DFS_km = KaplanMeier()\n",
    "DFS_km.fit(surv)\n",
    "print(DFS_km.summary)\n",
    "print(DFS_km)\n",
    "\n",
    "#Plot curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "DFS_km.plot()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#Estimate is basically reading off the curve at the respective time\n",
    "estimate = DFS_km.predict([0.002738,1,2,3,4,5,6,7,8,9,10])\n",
    "display(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer Specific Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Input DF\n",
    "surv = build_surv_obj(survival_type=\"CSS\", years=10, df_dict=df_dict)\n",
    "CSS_km = KaplanMeier()\n",
    "CSS_km.fit(surv)\n",
    "# print(CSS_km.summary)\n",
    "print(CSS_km)\n",
    "\n",
    "#Plot curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "CSS_km.plot()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
